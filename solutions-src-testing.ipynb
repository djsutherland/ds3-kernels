{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "<h2 style=\"color: red\">This is the solutions file!</h2>\n",
    "\n",
    "<span style=\"color: red\">We strongly recommend not looking at this file, which contains the solutions to all the exercises, until after the practical is over. Use [`testing.ipynb`](testing.ipynb) instead.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\DeclareMathOperator{\\E}{\\mathbb E}\n",
    "\\DeclareMathOperator{\\Var}{Var}\n",
    "\\DeclareMathOperator{\\Tr}{Tr}\n",
    "\\DeclareMathOperator{\\MMD}{MMD}\n",
    "\\DeclareMathOperator{\\MMDhat}{\\widehat{MMD}}\n",
    "\\newcommand{\\PP}{\\mathbb{P}}\n",
    "\\newcommand{\\QQ}{\\mathbb{Q}}\n",
    "\\newcommand{\\h}{\\mathcal H}\n",
    "$This is the second part of the practical components of the tutorial, on two-sample testing.\n",
    "\n",
    "It doesn't overlap too much with [the ridge regression section](ridge.ipynb), but we're going to assume you've done (at least the first parts of) that one first.\n",
    "\n",
    "This section is an updated version of (most of) [Heiko Strathmann](http://herrstrathmann.de/)'s [materials](https://github.com/karlnapf/ds3_kernel_testing) for a DS3 tutorial last year.\n",
    "\n",
    "Again, we'll use PyTorch; you might want a GPU for some optional bits at the end, but CPU on a laptop should be fine until then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    if not os.path.exists('data/blobs.npz'):\n",
    "        !git clone https://github.com/dougalsutherland/ds3-kernels\n",
    "        os.chdir('ds3-kernels')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook')\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm  # if you're in JupyterLab/etc and this doesn't work well\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "\n",
    "import ds3_support\n",
    "from ds3_support import as_tensors, LazyKernel, pil_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note\n",
    "\n",
    "Please ask me for help if you get stuck! Whether it's with a PyTorch thing, some other code thing, or especially something conceptual \u2013\u00a0that's what I'm here for. Solutions are also available in [`solutions-testing.ipynb`](solutions-testing.ipynb), but you're better off not rushing straight to those; I'm happy to help you try to help you work things out yourself, which is probably better for you. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting out with two-sample testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Generate the data...mean-shifted Gaussians\n",
    "N = 1000\n",
    "rs = np.random.RandomState(seed=0)\n",
    "X = rs.randn(N).astype(np.float32)\n",
    "Y = rs.randn(N).astype(np.float32) + 0.2\n",
    "np.savez('data/simple.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/simple.npz') as data:\n",
    "    X, Y = as_tensors(data['X'], data['Y'])\n",
    "    \n",
    "plt.hist(X.numpy(), alpha=.5, bins='auto', histtype='stepfilled')\n",
    "plt.hist(Y.numpy(), alpha=.5, bins='auto', histtype='stepfilled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: do $X$ and $Y$ come from the same distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by comparing the means of the samples.\n",
    "If the distributions are the same, their means are the same,\n",
    "so the expectation of this statistic would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_difference(X, Y, squared=False):\n",
    "    X, Y = as_tensors(X, Y)\n",
    "    assert len(X.shape) == len(Y.shape) == 1\n",
    "    \n",
    "    # TODO: compute mean difference of X and Y\n",
    "    result = X.mean() - Y.mean()  # SOLUTION\n",
    "    \n",
    "    return (result * result) if squared else result\n",
    "\n",
    "mean_difference(X, Y).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it's not zero...but of course we were never going to get *exactly* zero.\n",
    "\n",
    "Where do we draw the line?\n",
    "\n",
    "The classical statistical way to do this is to assume that $P_X = P_Y$ (the null hypothesis), and consider what the distribution of the test statistic would be in that case (the null distribution). If the number we observe would be very unlikely under the null distribution, then we \"reject the null\" and say that the two samples are different.\n",
    "\n",
    "One way to do this is called _permutation testing_. This is based on the observation that if $P_X = P_Y$, then we can shuffle the samples together, and that won't change the distribution of our shuffled $X'$ or $Y'$ (under the null). We can then compute what our test statistic would be if we did this a bunch of times, and this will give us an estimate of what the true null distribution is. If we do a lot of permutation samples, and our test statistic landed above, say, 99% of them, then we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def two_sample_permutation_test(\n",
    "    test_statistic, X, Y, num_permutations=1000, progress=True\n",
    "):\n",
    "    X, Y = as_tensors(X, Y)\n",
    "    assert len(X.shape) == len(Y.shape)\n",
    "    \n",
    "    orig_stat = test_statistic(X, Y)\n",
    "    \n",
    "    range_ = range(num_permutations)\n",
    "    if progress:\n",
    "        range_ = tqdm(range_)\n",
    "    \n",
    "    # concatenate samples together\n",
    "    Z = torch.cat([X, Y], 0)\n",
    "    \n",
    "    stats = []\n",
    "    n_X = X.shape[0]                                  # SOLUTION\n",
    "    for i in range_:\n",
    "        # TODO: permute samples and compute test statistic\n",
    "        np.random.shuffle(Z.numpy())                  # SOLUTION\n",
    "        this_stat = test_statistic(Z[:n_X], Z[n_X:])  # SOLUTION\n",
    "        stats.append(this_stat)\n",
    "    return orig_stat, torch.stack(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_permutation_samples(statistic, null_samples, ax=None,\n",
    "                             from_zero=False, one_sided=False, alpha=1,\n",
    "                             level=.05):    \n",
    "    null_samples = np.asarray(null_samples)\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    ax.hist(null_samples, bins='auto', histtype='stepfilled',\n",
    "            label=\"Permutation samples\", alpha=alpha)\n",
    "    \n",
    "    if from_zero:\n",
    "        lo = 0\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level))\n",
    "    elif one_sided:\n",
    "        lo = np.min(null_samples)\n",
    "        # should be -inf, but that takes more code\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level))\n",
    "    else:\n",
    "        lo = np.percentile(null_samples, 100 * level / 2)\n",
    "        hi = np.percentile(null_samples, 100 * (1 - level / 2))\n",
    "    \n",
    "    ax.axvspan(lo, hi, fc='b', alpha=.25, label=\"95% region\")\n",
    "    \n",
    "    ax.axvline(x=statistic, c='r', lw=2, label=\"Actual statistic\")\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "    ax.set_xlabel(\"Test statistic value\")\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    \n",
    "    if from_zero:\n",
    "        ax.set_xlim(0, ax.get_xlim()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(mean_difference, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the test statistic, there might be many other ways to compute the null distribution.\n",
    "Not all of them work via sampling it directly like this \u2013 which can be pretty expensive, e.g. 500 times as expensive as the test in the first place....\n",
    "\n",
    "For example, for our mean difference test statistic, if we assume that $X$ and $Y$ are each Gaussian, the distribution of the statistic is also Gaussian \u2013\u00a0as we can see, at least roughly, in this plot. \n",
    "We might be even able to analytically work out the parameters of this distribution.\n",
    "\n",
    "If you haven't realized, this is basically a [t-test](https://en.wikipedia.org/wiki/Student%27s_t-test), where we assume that we know that both distributions have the same variance so we can distinguish them via solely looking at their mean. (In this particular case, that's true....)\n",
    "\n",
    "As it will be useful for kernel based test statistics later, we will also look at the distribution of a squared test statistic, in order to make it strictly positive, but still tend to zero if the two distributions are the same.\n",
    "Naturally, the average of squared Gaussian random variables has a chi-square distribution -- something that can be used for the kernel tests that we introduce later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_mean_diff = functools.partial(mean_difference, squared=True)\n",
    "\n",
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(squared_mean_diff, X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative distribution via explicit simulation\n",
    "\n",
    "The test statistic also has a distribution under the alternative hypothesis ($P_X \\neq P_Y$); our computed test statistic is only a single sample from this distribution.\n",
    "\n",
    "Unfortunately, it's not easy to look at this alternative distribution in practice,\n",
    "unless we have a way to generate more data.\n",
    "But let's take a look in a synthetic case where we know the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3); torch.manual_seed(0)\n",
    "\n",
    "# This is a case when the null hypothesis is false:\n",
    "N = 200\n",
    "sample_X = lambda: torch.randn(N)\n",
    "sample_Y = lambda: torch.randn(N) + 0.3\n",
    "\n",
    "# Do the normal testing thing, where we have one sample from each.\n",
    "X = sample_X()\n",
    "Y = sample_Y()\n",
    "\n",
    "# single sample from the alternative + null estimates\n",
    "stat, perm_samples = two_sample_permutation_test(squared_mean_diff, X, Y, num_permutations=500)\n",
    "\n",
    "statistics_alt = np.zeros(500)\n",
    "# TODO: fill statistics_alt with samples from the alternative distribution\n",
    "for i in tqdm(range(statistics_alt.shape[0])):                     # SOLUTION\n",
    "    statistics_alt[i] = squared_mean_diff(sample_X(), sample_Y())  # SOLUTION\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "plot_permutation_samples(stat, perm_samples, ax=ax, from_zero=True, alpha=.5)\n",
    "ax.hist(statistics_alt, alpha=0.5, bins='auto',\n",
    "        label='Alternative samples', histtype='stepfilled');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, even though most of the alternative distribution far larger than the null, we can be unlucky: there are a significant portion of datasets for which the test would not reject the null simply by chance -- we only have a single draw from the alternative.\n",
    "With this fixed seed, we got lucky, but a decent portion of the time we wouldn't reject the null, even though it's not true.\n",
    "\n",
    "**Optional:** Play with the sample size, or the amount of difference between $X$ and $Y$, and see how this impacts the position of the alternative distribution (and as such the test power)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** Actually generate more data from the null distribution, also. How well does the permutation distribution approximate the true null? How does this change with smaller / larger `N`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# gaussian vs laplace with same mean / variance\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(2)\n",
    "N = 300\n",
    "X = stats.norm.rvs(size=N).astype(np.float32)\n",
    "Y = stats.laplace.rvs(size=N, scale=np.sqrt(.5)).astype(np.float32)\n",
    "\n",
    "np.savez('data/almost_simple.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/almost_simple.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])\n",
    "\n",
    "plt.hist(X.numpy(), alpha=.5, bins='auto', histtype='stepfilled')\n",
    "plt.hist(Y.numpy(), alpha=.5, bins='auto', histtype='stepfilled');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the mean won't cut it this time. Maybe the standard deviation?\n",
    "\n",
    "**Exercise:** Implement a two-sample test statistic based on both the mean and the standard deviation, e.g. $(\\mu_X - \\mu_Y)^2 + (\\sigma_X - \\sigma_Y)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_stat(X, Y):\n",
    "    X, Y = as_tensors(X, Y)\n",
    "    assert len(X.shape) == len(Y.shape) == 1\n",
    "    # TODO: implement\n",
    "    \n",
    "    return (X.mean() - Y.mean()) ** 2 + (X.std() - Y.std()) ** 2\n",
    "\n",
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(mean_std_stat, X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No good. Of course, it's possible we were just unlucky (as we saw could happen above), but this test clearly can't distinguish between distributions with the same mean and variance in any case \u2013\u00a0so it's time to move onto something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Mean Discrepancy\n",
    "\n",
    "As we saw in the lectures this morning, the MMD can be thought of as either\n",
    "$$\n",
    "\\MMD(\\PP, \\QQ)\n",
    "= \\sup_{f \\in \\mathcal H : \\lVert f \\rVert_{\\mathcal H} \\le 1} \\E_{X \\sim \\PP}[f(X)] - \\E_{Y \\sim \\QQ}[ f(Y) ]\n",
    "$$\n",
    "or, more relevantly right now,\n",
    "$$\n",
    "\\MMD(\\PP, \\QQ)\n",
    "=\n",
    "\\lVert\n",
    "\\E_X[ \\varphi(X) ]\n",
    "- \\E_Y[ \\varphi(Y) ]\n",
    "\\rVert_{\\mathcal H}\n",
    "$$\n",
    "where $\\varphi : \\mathcal X \\to \\mathcal H$ is the feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take $\\varphi(x) = x$, corresponding to a linear kernel, then our squared difference in means is exactly the squared MMD. But we can do a lot better by picking a different kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel(LazyKernel):\n",
    "    def _compute(self, A, B):\n",
    "        return A @ B.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** does the difference in means + standard deviations that we used above correspond to an MMD with some kernel? If so, what kernel? If not, is there a kernel MMD that can distinguish the same set of distributions as that can?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from lecture that a *characteristic* kernel, like the Gaussian, can distinguish *any* pair of distributions (with enough samples). Let's start with the Gaussian; go ahead and bring over your implementation from the other notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFKernel(LazyKernel):\n",
    "    def __init__(self, *parts, sigma=1):\n",
    "        super().__init__(*parts)\n",
    "        self.sigma = sigma\n",
    "        self.const_diagonal = 1  # Says that k(x, x) = 1 for any x\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n",
    "    def _precompute(self, A):                                            # SOLUTION\n",
    "        # Squared norms of each data point                               # SOLUTION\n",
    "        return [torch.einsum(\"ij,ij->i\", A, A)]                          # SOLUTION\n",
    "                                                                         # SOLUTION\n",
    "    def _compute(self, A, A_sqnorms, B, B_sqnorms):                      # SOLUTION\n",
    "        D2 = A_sqnorms[:, None] + B_sqnorms[None, :] - 2 * (A @ B.t())   # SOLUTION\n",
    "        return torch.exp(D2 / (-2 * self.sigma ** 2))                    # SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Matrix` classes\n",
    "\n",
    "There's also one handy thing about these `LazyKernel` classes that we didn't need in the ridge regression setting, but might be useful here. (You don't have to use them, but you can.)\n",
    "\n",
    "if you do `K.XY_m` (or `K.YY_m` or `K.matrix(0, 1)`, etc), then you get a special `ds3_support.kernels.Matrix` subclass. This implements \u2013\u00a0and caches \u2013\u00a0various operations you might need. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make X, Y into [n, 1] matrices instead of just vectors\n",
    "if len(X.shape) == 1:\n",
    "    X = X[:, None]\n",
    "if len(Y.shape) == 1:\n",
    "    Y = Y[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = RBFKernel(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XY_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  '.join(m for m in dir(K.XY_m) if not m.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('  '.join(m for m in dir(K.XX_m) if not m.startswith('_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX_m.mean(), K.XX_m.offdiag_mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the second half of [`ds3_support/kernels.py`](ds3_support/kernels.py) to see how they're implemented / what options there are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMD estimators\n",
    "\n",
    "Okay, enough admiring my beautiful code. Remember that we have\n",
    "\\begin{align}\n",
    "\\MMD^2(\\PP, \\QQ)\n",
    "  &= \\lVert \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ] \\rVert_\\h^2\n",
    "\\\\&= \\langle\n",
    "        \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ],\n",
    "        \\E_X[ \\varphi(X) ] - \\E_Y[ \\varphi(Y) ]\n",
    "     \\rangle_\\h\n",
    "\\\\&= \\langle \\E_X[ \\varphi(X) ], \\E_X[ \\varphi(X) \\rangle_\\h\n",
    "   + \\langle \\E_Y[ \\varphi(Y) ], \\E_Y[ \\varphi(Y) \\rangle_\\h\n",
    "   - 2 \\langle \\E_X[ \\varphi(X) ], \\E_Y[ \\varphi(Y) ] \\rangle_\\h\n",
    "\\\\&= \\E_{X, X', Y, Y'}\\left[\n",
    "     \\langle \\varphi(X), \\varphi(X') \\rangle_\\h\n",
    "   + \\langle \\varphi(Y), \\varphi(Y') \\rangle_\\h\n",
    "   - 2 \\langle \\varphi(X), \\varphi(Y) \\rangle_\\h\n",
    "   \\right]\n",
    "\\\\&= \\E_{X, X', Y, Y'}\\left[\n",
    "     k(X, X')\n",
    "   + k(Y, Y')\n",
    "   - 2 k(X, Y)\n",
    "   \\right]\n",
    ".\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a pretty natural idea for how to estimate the MMD. Well, three ideas of varying amounts of naturalness:\n",
    "\n",
    "The *biased* estimator (exactly the MMD between the empirical distributions) is, if we have $m$ samples from $X$ and $n$ from $Y$:\n",
    "$$\n",
    "\\MMDhat_b^2(X, Y)\n",
    "= \\frac{1}{m^2} \\sum_{i=1}^m \\sum_{j=1}^m k(X_i, X_j)\n",
    "+ \\frac{1}{n^2} \\sum_{i=1}^n \\sum_{j=1}^n k(Y_i, Y_j)\n",
    "- \\frac{2}{m n} \\sum_{i=1}^m \\sum_{j=1}^n k(X_i, Y_j)\n",
    ".$$\n",
    "But this term has an bias due to the $k(X_i, X_i)$ and $k(Y_i, Y_i)$ terms. (You can tell that it's biased because we can have $\\MMD^2(\\PP, \\QQ) = 0$, but we can have $\\MMDhat^2(X, Y) > 0$ even when $\\PP = \\QQ$ but we can't have $\\MMDhat^2(X, Y) < 0$. Thus $\\E \\MMDhat^2(X, Y) > 0$.\n",
    "\n",
    "The *unbiased* estimator gets rid of these terms:\n",
    "$$\n",
    "\\MMDhat_u^2(X, Y)\n",
    "= \\frac{1}{m (m-1)} \\sum_{i \\ne j}^m k(X_i, X_j)\n",
    "+ \\frac{1}{n (n-1)} \\sum_{i \\ne j}^n k(Y_i, Y_j)\n",
    "- \\frac{2}{m n} \\sum_{i=1}^m \\sum_{j=1}^n k(X_i, Y_j)\n",
    ".$$\n",
    "This makes it unbiased, and in fact it's the minimum variance unbiased estimator.\n",
    "\n",
    "The $U$-statistic estimator only works when $n = m$, and also takes out the $k(X_i, Y_i)$ terms, which gives you a slightly worse estimator:\n",
    "$$\n",
    "\\MMDhat_U^2(X, Y)\n",
    "= \\frac{1}{m (m-1)} \\sum_{i \\ne j}^m \\left(\n",
    "    k(X_i, X_j) + k(Y_i, Y_j) - 2 k(X_i, Y_j)\n",
    "  \\right)\n",
    ".$$\n",
    "The advantage is that $U$-statistics have been studied pretty thoroughly by statisticians, so we know things about their variance and whatnot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement all three estimators, as functions of a `LazyKernel(X, Y)`. Using the `Matrix` helpers, each can be literally one line, but you can implement it however you feel like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd2_biased(K):\n",
    "    pass  # TODO: implement, using K.XX, K.XY, K.YY (or _m)\n",
    "    return K.XX_m.mean() + K.YY_m.mean() - 2 * K.XY_m.mean()  # SOLUTION\n",
    "\n",
    "def mmd2_unbiased(K):\n",
    "    pass  # TODO: implement\n",
    "    return K.XX_m.offdiag_mean() + K.YY_m.offdiag_mean() - 2 * K.XY_m.mean()  # SOLUTION\n",
    "\n",
    "def mmd2_u_stat(K):\n",
    "    assert K.ns[0] == K.ns[1]\n",
    "    # TODO: implement\n",
    "    return K.XX_m.offdiag_mean() + K.YY_m.offdiag_mean() - 2 * K.XY_m.offdiag_mean()  # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_biased(RBFKernel(X, Y)), X, Y),\n",
    "    from_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_unbiased(RBFKernel(X, Y)), X, Y),\n",
    "    one_sided=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_u_stat(RBFKernel(X, Y)), X, Y),\n",
    "    one_sided=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They're all pretty much the same; when you're just doing permutation testing, the differences between them aren't super important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Run the test with a `LinearKernel`. Which of our estimators, if any, corresponds to the `mean_difference` statistic from before? (Do you understand why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "plot_permutation_samples(\n",
    "    *two_sample_permutation_test(lambda X, Y: mmd2_u_stat(LinearKernel(X, Y)), X, Y),\n",
    "    one_sided=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Answer: it's mmd2_biased, as you can see e.g. by\n",
    "# expanding k(x, y) = x y in the equation for the sum.\n",
    "# Here's the numerical check:\n",
    "K = LinearKernel(X, Y)\n",
    "torch.stack([\n",
    "    mean_difference(X.squeeze(1), Y.squeeze(1), squared=True),\n",
    "    mmd2_biased(K),\n",
    "    mmd2_unbiased(K),\n",
    "    mmd2_u_stat(K),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We're working on small datasets here so far, so this implementation of permutations is fine. But notice that we're recomputing the kernel matrix for each permutation, even though the matrices actually have all the same elements (just in jumbled-up order). Implement a faster way that doesn't involve this recomputation.\n",
    "\n",
    "Hint: this is easiest for the biased estimator, which you can write in a way amenable to [this kind of approach](https://pytorch.org/docs/stable/torch.html#torch.einsum). (The `LazyKernel.joint()` method, which concatenates all the kernel matrices together, might be useful.) Doing some algebra to work out a few slightly annoying correction terms, you can also do it similarly for the U-statistic estimator.\n",
    "\n",
    "If you're really, really careful, you can also make the unbiased estimator really fast (exploiting cache locality and things like that). We wrote about this in [this paper](https://arxiv.org/abs/1611.04488) (Section 3), and it's implemented in [Shogun](https://www.shogun-toolbox.org/). (Example usage in [here](https://nbviewer.jupyter.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/statistical_testing/mmd_two_sample_testing.ipynb); there should be an API example [here](http://shogun.ml/examples/latest/examples/statistical_testing/quadratic_time_mmd.html) but the Shogun website is currently broken.)\n",
    "\n",
    "Extra credit: empirically compare how our silly implementation here, your faster implementation, and the Shogun implementation scale as the dataset size increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override this function if you implement a better one yourself,\n",
    "# and the stuff later will run faster....\n",
    "# This just establishes an API for passing in a kernel directly,\n",
    "# which is a little clumsy to implement, but it's not any faster.\n",
    "def mmd2_permutations_slow(K, use_biased=True, num_permutations=1000, progress=True):\n",
    "    # Some fiddling to be able to use the same kernel with \"new\" data.\n",
    "    from copy import copy\n",
    "    K_copy = copy(K)\n",
    "    def mmd2_with_K(X, Y):\n",
    "        K_copy.change_part(0, X)\n",
    "        K_copy.change_part(1, Y)\n",
    "        return (mmd2_biased if use_biased else mmd2_unbiased)(K_copy)\n",
    "    \n",
    "    return two_sample_permutation_test(\n",
    "        mmd2_with_K, K.X, K.Y,\n",
    "        num_permutations=num_permutations, progress=progress)\n",
    "\n",
    "mmd2_permutations = mmd2_permutations_slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_samples(*mmd2_permutations_slow(K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "def mmd2_permutations(K, use_biased=True, permutations=1000, progress=None):\n",
    "    # progress is ignored\n",
    "    full_kernel = K.joint()\n",
    "    assert K.n_parts == 2\n",
    "    n_X, n_Y = K.ns\n",
    "    n = n_X + n_Y\n",
    "\n",
    "    if use_biased:\n",
    "        w_X = 1 / n_X\n",
    "        w_Y = -1 / n_Y\n",
    "    else:  # use U statistic\n",
    "        assert n_X == n_Y\n",
    "        w_X = 1\n",
    "        w_Y = -1\n",
    "\n",
    "    ws = torch.full((permutations + 1, n), w_Y,\n",
    "                    dtype=full_kernel.dtype, device=full_kernel.device)\n",
    "    ws[-1, :n_X] = w_X\n",
    "    for i in range(permutations):\n",
    "        ws[i, np.random.choice(n, n_X, replace=False)] = w_X\n",
    "\n",
    "    biased_ests = torch.einsum(\"pi,ij,pj->p\", ws, full_kernel, ws)\n",
    "    if use_biased:\n",
    "        ests = biased_ests\n",
    "    else:\n",
    "        # need to subtract \\sum_i k(X_i, X_i) + k(Y_i, Y_i) + 2 k(X_i, Y_i)\n",
    "        # first two are just trace, but last is harder:\n",
    "        is_X = ws > 0\n",
    "        X_inds = is_X.nonzero()[:, 1].view(permutations + 1, n_X)\n",
    "        Y_inds = (~is_X).nonzero()[:, 1].view(permutations + 1, n_Y)\n",
    "        del is_X, ws\n",
    "        cross_terms = K.take(Y_inds * n + X_inds).sum(1)\n",
    "        del X_inds, Y_inds\n",
    "        ests = (biased_ests - K.trace() + 2 * cross_terms) / (n_X * (n_X - 1))\n",
    "\n",
    "    est = ests[-1]\n",
    "    rest = ests[:-1]\n",
    "    return est, rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_permutation_samples(*mmd2_permutations(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative methods to estimate the null distribution\n",
    "\n",
    "There are many other way to get our hands on the distribution of the MMD test statistic under the null hypothesis.\n",
    "\n",
    "We know that asymptotically, the distribution is an infinite sum of Chi-square variables \u2013 but it's an infinite sum, and we don't even know what sum it is, so that doesn't help us too much.\n",
    "Techniques to approximate it include:\n",
    "\n",
    "* moment matching using a Gamma distribution: fast, but doesn't result in a consistent test.\n",
    "* a spectral approximation: using eigenvalues of the kernel matrix, we can estimate that infinite sum. Costly (cubic!) for large sample sets.\n",
    "* wild bootstrap: a technique used for correlated samples that is similar to permuting. (It's useful for e.g. testing an MCMC chain, though.)\n",
    "* linear time statistics: for those, one can often show that the null distribution is Gaussian, with a variance you can estimate in closed form.\n",
    "\n",
    "We won't cover any of these here, since they require some more details.\n",
    "The permutation test, while it can potentially be slow, is easy to understand and works for most applications (if implemented well).\n",
    "\n",
    "[This old Shogun notebook](https://nbviewer.jupyter.org/github/shogun-toolbox/shogun/blob/develop/doc/ipython-notebooks/statistical_testing/mmd_two_sample_testing.ipynb) contains many null approximation methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MMD witness function in the RKHS\n",
    "\n",
    "One nice feature of the MMD is that we can see where the density functions are different by looking at the (empirical) MMD witness function.\n",
    "Remember that\n",
    "\\begin{align}\n",
    "\\MMD(\\PP, \\QQ)\n",
    "  &= \\sup_{f : \\lVert f \\rVert_\\h \\le 1} \\E_{X \\sim \\PP} f(X) - \\E_{Y \\sim \\QQ} f(Y)\n",
    "\\\\&= \\sup_{f : \\lVert f \\rVert_\\h \\le 1} \\langle f, \\E_{X \\sim \\PP}[\\varphi(X)] - \\E_{Y \\sim \\QQ}[\\varphi(Y)] \\rangle_\\h\n",
    "\\end{align}\n",
    "and so\n",
    "$$\n",
    "f^* \\propto \\E_{X \\sim \\PP}[\\varphi(X)] - \\E_{Y \\sim \\QQ}[\\varphi(Y)]\n",
    ",$$\n",
    "which means (using $f^*(t) = \\langle f^*, \\varphi(t) \\rangle_\\h$)\n",
    "$$\n",
    "f^*(t) \\propto \\E_{X \\sim \\PP} k(X, t) - \\E_{Y \\sim \\QQ} k(Y, t)\n",
    ".$$\n",
    "\n",
    "We can estimate $f^*$ with empirical averages.\n",
    "\n",
    "**Thought exercises:**\n",
    "- What's the proportionality constant hidden by $\\propto$?\n",
    "- Does the constant matter?\n",
    "- Which MMD estimator, if any, does directly estimating $f^*$ correspond to?\n",
    "\n",
    "The points where $\\lvert f^*(t) \\rvert$ are large are where the MMD test considers $\\PP$ and $\\QQ$ to be the \"most different.\"\n",
    "Let's define a grid and evaluate the witness function on it to see where that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmd_witness(K, eval_pts):\n",
    "    assert K.n_parts == 2\n",
    "    K.append_part(eval_pts[:, None])  # now K.XZ compares X to eval_pts\n",
    "    \n",
    "    witness = 0\n",
    "    # TODO: estimate MMD witness function on grid_pts\n",
    "    witness = K.XZ.mean(0) - K.YZ.mean(0)  # SOLUTION\n",
    "    \n",
    "    K.drop_last_part()\n",
    "    witness = witness / mmd2_biased(K)  # to normalize; not necessary  # SOLUTION\n",
    "    return witness\n",
    "\n",
    "def plot_mmd_witness_1d(K, ax=None, grid_num=1000):\n",
    "    X, Y = [t.squeeze(1) for t in K.parts]\n",
    "    assert len(X.shape) == 1\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.hist(X.numpy(), alpha=.5, density=True, histtype='stepfilled', bins='auto')\n",
    "    ax.hist(Y.numpy(), alpha=.5, density=True, histtype='stepfilled', bins='auto')\n",
    "    ax.grid(False)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    lo, hi = ax2.get_xlim()\n",
    "    grid_pts = torch.linspace(lo, hi, grid_num)\n",
    "    ax2.plot(grid_pts.numpy(), mmd_witness(K, grid_pts).numpy(), color='k', lw=2)\n",
    "    ax2.set_xlim(lo, hi)\n",
    "    ax2.grid(False)\n",
    "    ax2.axhline(0, color='k', lw=0.5)\n",
    "    ax2.set_ylabel(\"Witness value\")\n",
    "    \n",
    "plot_mmd_witness_1d(RBFKernel(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the witness function is positive where X as a higher density than Y, and negative vice versa.\n",
    "It is zero where both densities match.\n",
    "Intuitively, the RKHS norm of this function can only be zero if the densities match everywhere, and it grows as the densities differ on more and more points in their support.\n",
    "Of course, this kind of visualization only works in low dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel learning\n",
    "\n",
    "After doing the ridge regression notebook, hopefully it doesn't surprise you that choosing the right kernel will be important here, too. But let's see what happens:\n",
    "\n",
    "**Thought exercise:** What do you expect the results of the next code block to look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sig in [.1, .001, 10]:\n",
    "    K = RBFKernel(X, Y, sigma=sig)\n",
    "    fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "    plot_permutation_samples(*mmd2_permutations(K), ax=a1, from_zero=True)\n",
    "    plot_mmd_witness_1d(K, ax=a2)\n",
    "    fig.suptitle(f\"$\\sigma$ = {sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you intuitively understand what happened here (especially the witness plots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, remember the median heuristic rule from before? That also works for two-sample testing, for many datasets. Let's try it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_distance(Z):\n",
    "    # TODO: compute the median distance among the stacked samples Z\n",
    "    # If you want to be fancy, add options to optionally subset if Z is big.\n",
    "    median_dist = torch.median(torch.pdist(Z))  # SOLUTION\n",
    "    return median_dist\n",
    "\n",
    "med = median_distance(torch.cat([X, Y], 0))\n",
    "K = RBFKernel(X, Y, sigma=med)\n",
    "\n",
    "fig, (a1, a2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "plot_permutation_samples(*mmd2_permutations(K), ax=a1, one_sided=True)\n",
    "plot_mmd_witness_1d(K, ax=a2)\n",
    "\n",
    "med.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the default 1 wasn't too far off, actually. The median heuristic worked great!\n",
    "\n",
    "But it doesn't always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A dataset where the median heuristic doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def sample_blobs(n=500, ratio=0.01, rows=10, cols=10, sep=10, rs=None):\n",
    "    rs = check_random_state(rs)\n",
    "    # ratio is eigenvalue ratio\n",
    "    correlation = (ratio - 1) / (ratio + 1)\n",
    "\n",
    "    # generate within-blob variation\n",
    "    mu = np.zeros(2)\n",
    "    sigma = np.eye(2)\n",
    "    X = rs.multivariate_normal(mu, sigma, size=n)\n",
    "\n",
    "    corr_sigma = np.array([[1, correlation], [correlation, 1]])\n",
    "    Y = rs.multivariate_normal(mu, corr_sigma, size=n)\n",
    "\n",
    "    # assign to blobs\n",
    "    X[:, 0] += rs.randint(rows, size=n) * sep\n",
    "    X[:, 1] += rs.randint(cols, size=n) * sep\n",
    "    Y[:, 0] += rs.randint(rows, size=n) * sep\n",
    "    Y[:, 1] += rs.randint(cols, size=n) * sep\n",
    "\n",
    "    return X.astype(np.float32), Y.astype(np.float32)\n",
    "\n",
    "X, Y = sample_blobs(rs=0)\n",
    "np.savez('data/blobs.npz', X=X, Y=Y)\n",
    "\n",
    "X, Y = sample_blobs(rs=1)\n",
    "np.savez('data/blobs2.npz', X=X, Y=Y)\n",
    "\n",
    "X, Y = sample_blobs(rs=2, rows=1, cols=1)\n",
    "np.savez('data/blobs_single.npz', X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "\n",
    "with np.load('data/blobs.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])\n",
    "ax1.scatter(*X.t(), alpha=.5, label='X')\n",
    "ax1.scatter(*Y.t(), alpha=.5, label='Y')\n",
    "# ax1.legend()\n",
    "ax1.set_title(\"Blobs\")\n",
    "\n",
    "with np.load('data/blobs_single.npz') as d:\n",
    "    ax2.scatter(*d['X'].T, alpha=0.5, label='X')\n",
    "    ax2.scatter(*d['Y'].T, alpha=0.5, label='Y')\n",
    "ax2.legend()\n",
    "ax2.set_title(\"One blob (zoomed in)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two distributions are clearly very different. But the scale at which they're different is much smaller than the median distance, which looks mostly at global structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "med = median_distance(torch.cat([X, Y], 0))\n",
    "K = RBFKernel(X, Y, sigma=med)\n",
    "\n",
    "plot_permutation_samples(*mmd2_permutations(K), from_zero=True)\n",
    "med.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a machine learner (and maybe forgetting about the end of the ridge regression notebook from earlier), your first instinct might be to try a bunch of different values and see which one works best.\n",
    "\n",
    "One question is what \"best\" means. Well, we can try doing cross-validation and seeing which kernel choice rejects the null most often.\n",
    "\n",
    "Note that we need to do this kernel fitting on *separate* data than conducting our final test. Otherwise we'd overfit, and be too likely to reject the null by post-hoc picking one that was big just due to chance. In real life, if `X` and `Y` were all we had, we'd have to split them up (and guess at how much to split them by...) into a \"training\" and a \"testing\" set.\n",
    "\n",
    "**Quick thought exercise:** What are the tradeoffs when picking a bigger training set versus a bigger testing set? When would one be a better choice? Can you try multiple values and see?\n",
    "\n",
    "**Answer:** If the kernel parameter is relatively unimportant, or easy to pick, then a bigger testing set will give you more of a chance to identify the difference in the data. But a bigger training set will give you more ability to pick the best kernel. Unfortunately, you can only try one split (unless you correct for multiple testing). <!-- SOLUTION -->\n",
    "\n",
    "Because I'm generous, I'm going to provide you with another set of samples to learn a kernel on. Happy birthday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/blobs2.npz') as d:\n",
    "    X_train, Y_train = as_tensors(d['X'], d['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** implement cross-validation or a similar scheme to estimate kernel power. [`sklearn.model_selection`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) has some useful helpers if you want.\n",
    "\n",
    "You'll need to:\n",
    "- For each $\\sigma$ in a list of candidates:\n",
    "  - Split `X_train`, `Y_train` into subsets somehow (cross-validation or similar).\n",
    "  - For each split:\n",
    "    - Compute the $p$-value of a test based on the return value from `mmd2_permutations`.\n",
    "      (You might want to pass `progress=False` and only use an outer progress bar, for cleanliness.)\n",
    "    - Check whether the test rejected the null.\n",
    "  - Save the portion of times it rejected the null.\n",
    "- Compare (plot!) the power of each $\\sigma$.\n",
    "- Choose the best one, and use it to test on `X` and `Y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to give us an idea of which kernel parameter is better. Unfortunately, this approach is quite costly and noisy. In reality, we would also want to use more folds, repeat the cross-validation, and try out more kernel parameters (play with it and you will see). It's also tough to differentiate, so scaling to more complex kernel classes seems quite challenging.\n",
    "\n",
    "We could try one of the faster approximations for the null distribution mentioned above, for example the moment matching Gamma approach. This would speed up the procedure, but, the Gamma approximation has its limits in how accurate it is, especially for extreme values of the kernel.\n",
    "\n",
    "Instead, we'll do something better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy criterion for kernel power\n",
    "\n",
    "It turns out that under the alternative $\\PP \\ne \\QQ$,\n",
    "$\\MMDhat_U^2$ is asymptotically normal,\n",
    "$$\n",
    "\\frac{\\MMDhat_U^2(X, Y) - \\MMD^2(\\PP, \\QQ)}{\\sqrt{\\Var_{X' \\sim \\PP^m, Y' \\sim \\QQ^m}\\left[ \\MMDhat_U^2(X', Y') \\right]}}\n",
    "\\stackrel{D}{\\to}\n",
    "\\mathcal N(0, 1)\n",
    ".$$\n",
    "Call that thing in the denominator,\n",
    "which is the variance of $\\MMDhat_U^2$ given samples of size $m$ from $\\PP$ and $\\QQ$,\n",
    "$V_m(\\PP, \\QQ)$.\n",
    "\n",
    "Under the null hypothesis $\\PP = \\QQ$,\n",
    "we have that $m \\MMDhat_U^2(X, Y)$ converges to some gross distribution (an infinite mixture of chi-squares).\n",
    "Let $c_\\alpha$ be the $(1-\\alpha)$th quantile of that distribution,\n",
    "the rejection threshold;\n",
    "we don't know that exactly,\n",
    "but we can estimate it with permutation testing as $\\hat c_\\alpha$.\n",
    "In this framework, note that $c_\\alpha$ depends on $\\PP$ and $\\QQ$ (and the kernel in the MMD)\n",
    "but it *doesn't* depend on $m$,\n",
    "and in the usual setting we have\n",
    "$\\hat c_\\alpha \\to c_\\alpha$.\n",
    "\n",
    "So, the power of our test \u2013\u00a0the probability of rejecting when $\\PP \\ne \\QQ$ \u2013 is just\n",
    "\n",
    "\\begin{align}\n",
    "     \\Pr\\left( m \\MMDhat_U^2(X, Y) > \\hat c_\\alpha \\right)\n",
    "  &= \\Pr\\left(\n",
    "       \\frac{\\MMDhat_U^2(X, Y) - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "     > \\frac{\\hat c_\\alpha / m - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "   \\right)\n",
    "\\\\&\\to 1 - \\Phi\\left(\n",
    "        \\frac{c_\\alpha / m - \\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    \\right)\n",
    "\\\\&= \\Phi\\left(\n",
    "      \\frac{\\MMD^2(\\PP, \\QQ)}{\\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    - \\frac{c_\\alpha}{m \\sqrt{V_m(\\PP, \\QQ)}}\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\n",
    "We can then maximize the power of the test by maximizing the thing inside the parentheses.\n",
    "\n",
    "Now, it turns out that for a fixed kernel, $V_m$ is $\\mathcal{O}(1 / m)$, and $\\MMD^2$ and $c_\\alpha$ are constants. So the first term is $\\mathcal{O}\\left(\\sqrt{m}\\right)$ and the second is $\\mathcal{O}\\left(1 / \\sqrt{m}\\right)$ \u2013\u00a0which means we should be able to just ignore the second term when $m$ is reasonably large.\n",
    "\n",
    "That means we need to choose $k$ to maximize $\\MMD^2(\\PP, \\QQ) / \\sqrt{V_m(\\PP, \\QQ)}$. We know how to estimate $\\MMD^2$. Turns out you can also estimate $V_m$ in the same quadratic time: see [this note](https://arxiv.org/abs/1906.02104).\n",
    "\n",
    "The note is 12 pages of dense algebra, so...I'll be here when you're done.\n",
    "\n",
    "(Don't actually read it.)\n",
    "\n",
    "**Bonus points:** Find a mistake in the algebra in the note. I checked it three times, but I wouldn't be at all surprised if there are still mistakes!\n",
    "\n",
    "**Big bonus points:** Figure out a way to simplify the derivation. Seriously, huge bonus points.\n",
    "\n",
    "Anyway, even the _expression_ for the estimator is too disgusting to write here. But I implemented it for you, in [`ds3_support.mmd2_u_stat_variance`](ds3_support/mmd.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ds3_support import mmd2_u_stat_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There, I've done the hard part for you. (Also, PyTorch is doing the really hard part, which is differentiating the terrible expression.)\n",
    "\n",
    "**Exercise:** Compare a fixed set of kernels, the different bandwidths on the blobs data, using this criterion. Does it give you the same choice as cross-validation? How many thousands of times faster is it?\n",
    "\n",
    "This is an unbiased estimator, so it might be negative, or zero! Make sure you don't divide by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** We know how to estimate $c_\\alpha$ \u2013\u00a0with permutation testing \u2013 so we could optimize the more-complete criterion, too. Does that give you different answers? What about if $m$ is fairly small?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent to optimize the power criterion\n",
    "\n",
    "You know what the deal is by now:\n",
    "\n",
    "**Exercise:** Instead of doing a grid search, optimize $\\lambda$ with gradient descent.\n",
    "\n",
    "Does it always go to the same place, or can you get stuck in local minima?\n",
    "\n",
    "**Optional:** If you implemented a more efficient `mmd2_permutations`, and were a little careful about how you implemented it, you might even be able to differentiate your threshold estimate $\\hat c_\\alpha$. How does that compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN criticism with an ARD kernel\n",
    "\n",
    "You may have heard of Generative Adversarial Networks, GANs.\n",
    "Assume someone came to you with a new GAN, which generates some realistic looking images, which they say are indistinguishable from the images used to train the GAN.\n",
    "\n",
    "Actually, let's assume that person [said](https://arxiv.org/abs/1606.03498):\n",
    "> On MTurk, annotators were able to distinguish\n",
    "samples in 52.4% of cases (2000 votes total), where 50% would be obtained by random\n",
    "guessing. Similarly, researchers in our institution were not able to find any artifacts that would allow\n",
    "them to distinguish samples.\n",
    "\n",
    "Let's see what the kernel MMD thinks about this.\n",
    "We will use a slightly different kernel than before: an ARD (\"automatic relevance determination\") kernel.\n",
    "You can think of this kernel as scaling each of the $D$ input dimensions by a different parameter, and then uses a standard Gaussian kernel (unit bandwidth) on top of that.\n",
    "$$ k(x,y) = \\exp\\left(-\\frac12 \\sum_{d=1}^D \\left( s_d x_d - s_d y_d \\right)^2\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARDKernel(LazyKernel):\n",
    "    def __init__(self, *parts, scales=1):\n",
    "        super().__init__(*parts)\n",
    "        assert len(self.X.shape) == 2\n",
    "        d = self.X.shape[1]\n",
    "        \n",
    "        scales, = self.as_tensors(scales)\n",
    "        if scales.shape == ():\n",
    "            scales = scales.repeat(d)\n",
    "        else:\n",
    "            assert scales.shape == (d,)\n",
    "\n",
    "        self.scales = scales\n",
    "        self.const_diagonal = 1  # Says that k(x, x) = 1 for any x\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n",
    "    def _precompute(self, A):                      # SOLUTION\n",
    "        s = A * self.scales[None, :]               # SOLUTION\n",
    "        return [s, torch.einsum(\"id,id->i\", s, s)] # SOLUTION\n",
    "    \n",
    "    def _compute(self, A, sc_A, sc_A_sqnorms, B, sc_B, sc_B_sqnorms):  # SOLUTION\n",
    "        D2 = (sc_A_sqnorms[:, None] + sc_B_sqnorms[None, :]            # SOLUTION\n",
    "              - 2 * (sc_A @ sc_B.t()))                                 # SOLUTION\n",
    "        return torch.exp(-0.5 * D2)                                    # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that ARDKernel at least agrees with RBFKernel in the special case:\n",
    "Xtmp = torch.randn(3, 4)\n",
    "sigma = torch.exp(torch.randn(()))\n",
    "(ARDKernel(Xtmp, scales=1/sigma).XX\n",
    " - RBFKernel(Xtmp, sigma=sigma).XX).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/gan-samples.npz') as d:\n",
    "    X, Y = [t.reshape(-1, 784).float() for t in as_tensors(d['X'], d['Y'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(\"X (true MNIST) samples:\")\n",
    "display(pil_grid(X[:40].reshape(-1, 1, 28, 28), nrow=20))\n",
    "display(\"Y (GAN) samples:\")\n",
    "display(pil_grid(Y[:40].reshape(-1, 1, 28, 28), nrow=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How does a median-heuristic RBF kernel perform on this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Split the data, and optimize an ARD kernel with gradient descent to compare thesem; then run the test on the other half of the data. Does it do better?\n",
    "\n",
    "Can you interpret the ARD weights `K.scales`?\n",
    "\n",
    "We can't do the 1d witness function plot, but can you think of a different way to figure out \"where\" the distributions are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence testing\n",
    "\n",
    "So far, we've been asking whether $\\PP = \\QQ$ for two different distributions. Sometimes we'd rather do a related problem, *independence testing*. That is, we have paired data $\\{ (X_i, Y_i) \\}_{i=1}^n \\sim \\PP_{XY}^n$, and we want to know whether $X$ and $Y$ are independent or not.\n",
    "\n",
    "By definition, this is the same thing as asking whether $\\PP_{XY} = \\PP_X \\times \\PP_Y$, the product of the marginal distributions. But we know how to do that: if we use a characteristic kernel on the joint space, they're independent if and only if $\\MMD(\\PP_{XY}, \\PP_{X} \\times \\PP_Y) = 0$.\n",
    "The quantity $\\MMD(\\PP_{XY}, \\PP_{X} \\times \\PP_Y)^2$  is also called the HSIC (\"Hilbert-Schmidt Independence Criterion\"). \n",
    "\n",
    "Usually we choose a kernel $k$ for $X$ and a kernel $\\ell$ for $L$, and use the _product kernel_ on the joint space:\n",
    "$$\n",
    "  (k \\times \\ell)( (x, y), (x', y') )\n",
    "  = k(x, x') \\ell(y, y')\n",
    ".$$\n",
    "It turns out you can estimate it like this:\n",
    "$$\n",
    "\\frac{1}{n^2} \\Tr\\left( H K H L \\right)\n",
    ",$$\n",
    "where $K$ is the kernel matrix on the $X$ samples,\n",
    "$L$ is the kernel matrix on the $L$ samples,\n",
    "and $H = I - \\frac1n \\mathbf{1} \\mathbf{1}^T$ is the _centering matrix_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Implement the HSIC estimator. You can do it literally as written here, with an H matrix and taking matrix products and so on. Or, if you want a more efficient implementation \u2013\u00a0$\\mathcal{O}(n^2)$ instead of $\\mathcal{O}(n^3)$ \u2013 you can notice a few things:\n",
    "\n",
    "- If you expand out what $H K H$ does, you can easily implement it in quadratic time.\n",
    "- $\\Tr(A B) = \\sum_i ( A B )_{ii} = \\sum_i \\sum_j A_{ij} B_{ji}$, which you can easily implement in quadratic time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _as_kernel_matrices(K, L):\n",
    "    if isinstance(K, LazyKernel):\n",
    "        K = K.XX\n",
    "    if isinstance(L, LazyKernel):\n",
    "        L = L.XX\n",
    "    K, L = as_tensors(K, L)\n",
    "    assert len(K.shape) == len(L.shape) == 2\n",
    "    assert K.shape[0] == K.shape[1] == L.shape[0] == L.shape[1]\n",
    "    return K, L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_est(K, L):\n",
    "    K, L = _as_kernel_matrices(K, L)\n",
    "    \n",
    "    # TODO: implement the HSIC estimator\n",
    "    return hsic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a test, we'll also need to do permutations. Note that permutations are slightly different from before: we'll need to permute $K$ and $L$ *separately*, so that $\\{(x_i, y_i)\\}$ becomes $\\{(x_i, y_j)\\}$, and any possible dependence between them is broken.\n",
    "\n",
    "Here's some framework for computing permuations for the HSIC estimator. If you want to do it a faster way, go ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_permutations(K, L, num_permutations=1000, progress=True):\n",
    "    K, L = _as_kernel_matrices(K, L)\n",
    "\n",
    "    est = hsic_est(K, L)\n",
    "\n",
    "    range_ = range(num_permutations)\n",
    "    if progress:\n",
    "        range_ = tqdm(range_)\n",
    "    \n",
    "    stats = []\n",
    "    for i in range_:\n",
    "        pass # TODO: permute the entries of K and L and call hsic_est\n",
    "    \n",
    "    return two_sample_permutation_test(\n",
    "        mmd2_with_K, K.X, K.Y,\n",
    "        num_permutations=num_permutations, progress=progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on some simple data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "def sample_hsic(n, angle, sigma=0.2, offset=1):\n",
    "    n4 = int(n/4)\n",
    "    N = np.random.randn(n4, 2)*sigma\n",
    "    S = np.random.randn(n4, 2)*sigma\n",
    "    E = np.random.randn(n4, 2)*sigma\n",
    "    W = np.random.randn(n4, 2)*sigma\n",
    "    \n",
    "    N[:,1] += offset\n",
    "    S[:,1] -= offset\n",
    "    W[:,0] -= offset\n",
    "    E[:,0] += offset\n",
    "    \n",
    "    R = np.array([[np.cos(angle), -np.sin(angle)],[np.sin(angle), np.cos(angle)]])\n",
    "    A = R.dot(np.vstack((N,S,W,E)).T).T\n",
    "    \n",
    "    A = A.astype(np.float32)\n",
    "    return A[:, 0], A[:, 1]\n",
    "\n",
    "N = 200\n",
    "np.random.seed(0)\n",
    "X, Y = sample_hsic(n=N, angle=np.pi/12)\n",
    "np.savez(\"data/hsic.npz\", X=X, Y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/hsic.npz') as d:\n",
    "    X, Y = as_tensors(d['X'], d['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is some very simple dependence going on.\n",
    "The first thing a statistician would do is of course to check for correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between X and Y:\", np.corrcoef(X, Y)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation is very low.\n",
    "A kernel test for independence is more powerful, even with a bandwidth parameter chosen by the median heuristic.\n",
    "\n",
    "**Exercise:** Implement and visualize this test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "## European parliament documents translations and string kernels\n",
    "\n",
    "We will now do a slightly more elaborate test that involves some very mild NLP.\n",
    "More precisely we will analyze dependence between documents.\n",
    "We will use transcripts of the Canadian parliament's house debates, downloaded from [here](https://www.isi.edu/natural-language/download/hansard/).\n",
    "Those consist of pairs of French and English transcripts. Here's the end of two corresponding documents:\n",
    "\n",
    "> d until tomorrow at 2 p.m., pursuant to Standing Order 24(1).  \n",
    "> (The House adjourned at 6.41 p.m.) \n",
    "\n",
    "> main, \u00e0 10 heures, conform\u00e9ment \u00e0 l'article 24(1) du R\u00e8glement.  \n",
    "> (La s\u00e9ance est lev\u00e9e \u00e0 18 h 20.) \n",
    "\n",
    "Our question here is whether we can detect this supposedly strong dependence structure using the kernel HSIC.\n",
    "Note that this approach does not rely on attempting to translate the documents, but rather on comparing within-document structure. \n",
    "HSIC compares the self-similarity within the English documents with self-similarity of the French ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to construct a string kernel, a  \"bag of words\" kernel between documents $s$ and $t$,\n",
    "\n",
    "$$\n",
    "k(s,t) = \\frac{1}{|\\mathcal{W}|} \\phi(s)^\\top \\phi(t)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{W}$ consists of all words on all considered documents, and each element of $\\phi(x)\\in\\mathbb{N}^{|\\mathcal{W}|}$ contains the number of times that a particular word $w\\in\\mathcal{W}$ appears in $x$.\n",
    "\n",
    "Naturally, the kernel value will be larger, if a word appears in both documents many times.\n",
    "\n",
    "Efficient implementations of string kernels are rare (Shogun has many!), and often based on low-level dynamic programming concepts.\n",
    "Instead, we will here explicitly embed the documents into a feature space and compute (gram) matrix of inner products manually. \n",
    "Note that most string kernel implementations are much more efficient in computing the kernel since they don't do the feature space mapping explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords, from nltk.corpus.stopwords\n",
    "with open('data/stopwords-english.txt') as f:\n",
    "    en_stop = {x.strip() for x in f}\n",
    "with open('data/stopwords-french.txt') as f:\n",
    "    fr_stop = {x.strip() for x in f}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things faster, we're going to use the \"hashing trick\" to featurize this documents as bags of words \u2013\u00a0that is, each document is a count of how many times each unique word showed up. Rather than keeping track of a vocabulary, we use a hash function to map each word to a (probably) unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tarfile\n",
    "from scipy import sparse\n",
    "\n",
    "en_feats = {}\n",
    "fr_feats = {}\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "en_vec = HashingVectorizer(\n",
    "    analyzer=\"word\", decode_error='replace', stop_words=en_stop,\n",
    ")\n",
    "fr_vec = HashingVectorizer(\n",
    "    analyzer=\"word\", decode_error='replace', stop_words=fr_stop,\n",
    ")\n",
    "\n",
    "with tarfile.open('data/transcripts.tar.bz2') as tar:\n",
    "    for info in tqdm(tar):\n",
    "        fname = info.name\n",
    "        is_french = fname.endswith(\"f\")\n",
    "        is_english = fname.endswith(\"e\")\n",
    "        if not (is_english or is_french):\n",
    "            continue\n",
    "        \n",
    "        with tar.extractfile(info) as f:\n",
    "            if is_english:\n",
    "                en_feats[fname] = en_vec.transform([f.read()])\n",
    "            else:\n",
    "                fr_feats[fname] = fr_vec.transform([f.read()])\n",
    "#             (en_vec if is_en)\n",
    "#             (X if is_english else Y).append(content)\n",
    "\n",
    "assert len(en_feats) == len(fr_feats)\n",
    "names = {k[:-1] for k in en_feats}\n",
    "\n",
    "X = sparse.vstack([en_feats[k + 'e'] for k in names])\n",
    "Y = sparse.vstack([fr_feats[k + 'f'] for k in names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our features are enormous (and enormously sparse) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our PyTorch infrastructure isn't going to like these matrices with 1 million columns. We're only going to use a linear kernel, though, so we can just compute the kernel ourselves. (You can convert from these sparse matrix classes to numpy arrays with `toarray()`, but do it *after* computing the kernel matrix to avoid trying to allocate an enormous dense array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: compute kernels\n",
    "K_XX = (\n",
    "    # ...\n",
    "    X @ X.T   # SOLUTION\n",
    ").toarray().astype(np.float32)\n",
    "K_YY = (\n",
    "    # ...\n",
    "    Y @ Y.T   # SOLUTION\n",
    ").toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the HSIC test with these kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the English and French documents are not independent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
