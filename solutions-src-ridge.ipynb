{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\E}{\\mathbb E}\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- SOLUTION CELL -->\n",
    "<h2 style=\"color: red\">This is the solutions file!</h2>\n",
    "\n",
    "<span style=\"color: red\">We strongly recommend not looking at this file, which contains the solutions to all the exercises, until after the practical is over. Use [`practical.ipynb`](practical.ipynb) instead.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# This is put straight into practical.ipynb so it renders untrusted...\n",
    "from IPython.display import display, Markdown\n",
    "with open('README-setup.md') as f:\n",
    "    display(Markdown(f.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook')\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "\n",
    "from ds3_support import as_tensors, LazyKernel\n",
    "\n",
    "# download some datasets\n",
    "from torchvision.datasets import MNIST, Omniglot\n",
    "MNIST(root='data', download=True)\n",
    "Omniglot(root='data', download=True)\n",
    "\n",
    "# Feel free to read ahead while the datasets are downloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "Okay, time to get going.\n",
    "\n",
    "We're going to start with implementing kernel ridge regression. First, here's a toy dataset to test your code on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "def make_toy(n_pts, dist_seed=17, samp_seed=12):\n",
    "    drs = np.random.RandomState(seed=dist_seed)\n",
    "    inducing = drs.uniform(0, 1, size=50)\n",
    "    w = drs.normal(size=inducing.shape[0])\n",
    "    gamma = 1 / (2 * .05**2)\n",
    "    print(f\"||f||_H^2: {w @ rbf_kernel(inducing[:, None], gamma=gamma) @ w :.3}\")\n",
    "    \n",
    "    srs = np.random.RandomState(seed=samp_seed)\n",
    "    X = srs.uniform(0, 1, size=n_pts)\n",
    "    ymean = rbf_kernel(X[:, None], inducing[:, None], gamma=gamma) @ w\n",
    "    y = ymean + srs.normal(scale=.5, size=ymean.shape)\n",
    "    return X[:, None].astype(np.float32), y.astype(np.float32)\n",
    "\n",
    "X, y = make_toy(500)\n",
    "np.savez('data/ridge-toy.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/ridge-toy.npz') as f:\n",
    "    toy_X = f['X']\n",
    "    toy_y = f['y']\n",
    "toy_X_train, toy_X_test, toy_y_train, toy_y_test = model_selection.train_test_split(\n",
    "    toy_X, toy_y, train_size=200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(toy_X_train[:, 0], toy_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks like it could be well-modeled by a Gaussian RBF kernel. We'll need a fairly small bandwidth. See the minimum a little above 0.15 and the maximum a little less than 0.25? The Gaussian kernel still has a fair amount of influnce one bandwidth away, so we want to make the bandwidth a little smaller than that; say 0.05. So, first let's implement the kernel.\n",
    "\n",
    "<sup>I totally eyeballed that like I said, and definitely didn't just generate the true function in the first place from an RBF function with that bandwidth....</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first, let's implement an RBF kernel first. I've put some helper infrastructure in the `LazyKernel` class (in [`ds3_support.kernels`](ds3_support/kernels.py)) that will be especially useful later; for now, it's just a way to organize computing it. Here's an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel(LazyKernel):\n",
    "    def _compute(self, A, B):\n",
    "        return A @ B.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_compute` method computes the kernel between two inputs `A` and `B`. (`.t()` is PyTorch for taking a transpose; `@` is the nifty Python 3.6+ syntax for matrix multiplication.)\n",
    "\n",
    "The `LazyKernel` base class lets us use this in various ways. First, to find the kernel from one set of points to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(toy_X_train, toy_X_test)\n",
    "print(K)\n",
    "K.XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the X-to-X (`XX`) and Y-to-Y (`YY`) kernel matrices from the same object (which are the result of `_compute(X, X)` and `_compute(Y, Y)`). These aren't computed until you need them, but then they're cached after you use them the first time; this is why it's a `LazyKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want the kernel matrix for a dataset to itself, you can just not pass the second argument. Then K.XY won't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearKernel(toy_X_train).XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can pass `None`, which is a special value meaning \"use the first one.\" Then `XY` and so on will exist, but it knows to cache them appropriately.\n",
    "\n",
    "You can also pass three arguments; then there'll be `XZ`, etc. You can also access them with e.g. `K[0, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(toy_X_train, None, toy_X_test)\n",
    "print(K)\n",
    "K.YZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a slightly more complex kernel class, with some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialKernel(LazyKernel):\n",
    "    def __init__(self, X, *rest, degree=3, gamma=None, coef0=1):\n",
    "        super().__init__(X, *rest)\n",
    "        self.degree = degree\n",
    "        self.gamma = 1 / X.shape[1] if gamma is None else gamma\n",
    "        self.coef0 = coef0\n",
    "\n",
    "    def _compute(self, A, B):\n",
    "        XY = A @ B.t()\n",
    "        return (self.gamma * XY + self.coef0) ** self.degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also totally don't need to do this \u2013\u00a0you definitely won't notice the difference on this scale of data \u2013 but if you want to cache some computation for each dataset, you can use the `_precompute` interface. You can return a list of cached information in `_precompute`, which then get passed to `_compute` as `_compute(A, *A_precomputed, B, *B_precomputed)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAndSquareKernel(LazyKernel):\n",
    "    def _precompute(self, A):\n",
    "        return [A * A]\n",
    "    \n",
    "    def _compute(self, A, A_squared, B, B_squared):\n",
    "        return A @ B.t() + A_squared @ B_squared.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the Gaussian RBF kernel is\n",
    "$$k(x, y) = \\exp\\left( -\\frac{1}{2 \\sigma^2} \\lVert x - y \\rVert^2 \\right).$$\n",
    "Go ahead and implement that here. It might be helpful to recall that\n",
    "$$\\lVert x - y \\rVert^2 = \\lVert x \\rVert^2 + \\lVert y \\rVert^2 - 2 x^T y.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFKernel(LazyKernel):\n",
    "    def __init__(self, *parts, sigma=1):\n",
    "        super().__init__(*parts)\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n",
    "    def _precompute(self, A):                                            # SOLUTION\n",
    "        # Squared norms of each data point                               # SOLUTION\n",
    "        return [torch.einsum(\"ij,ij->i\", A, A)]                          # SOLUTION\n",
    "                                                                         # SOLUTION\n",
    "    def _compute(self, A, A_sqnorms, B, B_sqnorms):                      # SOLUTION\n",
    "        D2 = A_sqnorms[:, None] + B_sqnorms[None, :] - 2 * (A @ B.t())   # SOLUTION\n",
    "        return torch.exp(D2 / (-2 * self.sigma ** 2))                    # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation against scikit-learn's implementation (but it doesn't work in PyTorch, so don't just use it directly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.random.lognormal()\n",
    "K = RBFKernel(toy_X_train, toy_X_test, sigma=sigma)\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "gamma = 1 / (2 * sigma**2)  # sklearn uses this parameterization\n",
    "assert np.allclose(K.XX.numpy(), rbf_kernel(toy_X_train, gamma=gamma))\n",
    "assert np.allclose(K.XY.numpy(), rbf_kernel(toy_X_train, toy_X_test, gamma=gamma))\n",
    "assert np.allclose(K.YY.numpy(), rbf_kernel(toy_X_test, gamma=gamma))\n",
    "\n",
    "del rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing ridge regression\n",
    "\n",
    "Okay, now we can compute all kinds of kernels; let's actually use them for something. First, let's implement ridge regression. Remember that ridge regression is\n",
    "$$\n",
    "  \\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^n \\lVert f(X_i) - y_i \\rVert^2 + \\frac12 \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2 \n",
    ".$$\n",
    "The representer theorem tells us that $f(x) = \\sum_{i=1}^n \\alpha_i k(X_i, x)$,\n",
    "and then some calculus gives us\n",
    "$$\\alpha = (K + n \\lambda I)^{-1} y, \\tag{*}$$\n",
    "where $K_{ij} = k(X_i, X_j)$.\n",
    "\n",
    "There's also one more wrinkle: we're going to need to compute ridge regression for multiple outputs $y$ at once with the same $X$s. This just means solving the linear system for more than one $y$ at once; most software, including PyTorch, supports this built-in.\n",
    "\n",
    "To implement $\\textrm{(*)}$, there are (at least) two approaches.\n",
    "\n",
    "- [`torch.solve`](https://pytorch.org/docs/stable/torch.html#torch.solve) is a general-purpose matrix solver, which uses an [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition). This doesn't exploit the special structure of kernel matrices (namely that they're [positive-definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix)).\n",
    "\n",
    "- You could also use a [Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_factorization), which is the standard approach for positive-definite matrices, and will be somewhat faster. You could implement this with [`torch.cholesky`](https://pytorch.org/docs/stable/torch.html#torch.cholesky) and [`torch.cholesky_solve`](https://pytorch.org/docs/stable/torch.html#torch.cholesky_solve) \u2013\u00a0but unfortunately PyTorch hasn't implemented derivatives through `cholesky_solve` yet, and we're going to need that later. You can use [`torch.triangular_solve`](https://pytorch.org/docs/stable/torch.html#torch.triangular_solve) instead; Cholesky gives us $L$ such that $K + n \\lambda I = L L^T$, so you'll want $\\alpha = (L L^T)^{-1} y = L^{-T} (L^{-1} y)$. (Make sure to pass `upper=False` to `triangular_solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        reg_wt=1,\n",
    "        solver='cholesky',  # SOLUTION\n",
    "    ):\n",
    "        \"\"\"\n",
    "        reg_wt: The regularization weight lambda.\n",
    "        \"\"\"\n",
    "        self.reg_wt = reg_wt\n",
    "        self.solver = solver  # SOLUTION\n",
    "    \n",
    "    def fit(self, K_XX, y):\n",
    "        \"\"\"\n",
    "        Fit the ridge regression:\n",
    "        \n",
    "          K_XX: The training kernel matrix, of shape [n_train, n_train].\n",
    "          y: The training labels, of shape [n_train] or [n_train, n_labels].        \n",
    "        \"\"\"\n",
    "        K_XX, y = as_tensors(K_XX, y)\n",
    "        \n",
    "        assert len(K_XX.shape) == 2\n",
    "        self.n_train_ = K_XX.shape[0]\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            self.n_labels_ = None\n",
    "            y = y[:, None]\n",
    "        else:\n",
    "            assert len(y.shape) == 2\n",
    "            self.n_labels_ = y.shape[1]\n",
    "\n",
    "        assert K_XX.shape[1] == y.shape[0] == self.n_train_\n",
    "        \n",
    "        # TODO: find the solution, and save it in self.alpha_\n",
    "        # If you set anything\n",
    "        to_inv = K_XX + self.reg_wt * torch.eye(K_XX.shape[0])          # SOLUTION\n",
    "        if self.solver == 'lu':                                         # SOLUTION\n",
    "            # the LU-based solution                                     # SOLUTION\n",
    "            self.alpha_ = torch.solve(y, to_inv)[0]                     # SOLUTION\n",
    "        elif self.solver == 'cholesky':                                 # SOLUTION\n",
    "            # the cholesky solution (default)                           # SOLUTION\n",
    "            L = torch.cholesky(to_inv, upper=False)                     # SOLUTION\n",
    "            inner = torch.triangular_solve(y, L, upper=False).solution  # SOLUTION\n",
    "            self.alpha_ = torch.triangular_solve(                       # SOLUTION\n",
    "                inner, L, upper=False, transpose=True).solution         # SOLUTION\n",
    "        else:                                                           # SOLUTION\n",
    "            raise ValueError(f\"unknown solver {self.solver!r}\")         # SOLUTION\n",
    "    \n",
    "    def predict(self, K_test):\n",
    "        \"\"\"\n",
    "        Predict the labels of a test set.\n",
    "        \n",
    "          K_test: The train-to-test kernel matrix, shape [n_train, n_test].\n",
    "        \n",
    "        Return: the vector of test predictions, shape [n_test].\n",
    "        \"\"\"\n",
    "        K_test = torch.as_tensor(K_test)\n",
    "        assert len(K_test.shape) == 2\n",
    "        assert K_test.shape[0] == self.n_train_\n",
    "        \n",
    "        # TODO: set preds to shape [n_test, n_labels]\n",
    "        preds = K_test.t() @ self.alpha_     # SOLUTION\n",
    "        \n",
    "        # return a vector if we were fit with a vector, matrix otherwise\n",
    "        return preds.squeeze(1) if self.n_labels_ is None else preds\n",
    "    \n",
    "    def mses(self, K_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes the mean squared error of each output for a test set.\n",
    "        \n",
    "          K_test: The train-to-test kernel matrix, shape [n_train, n_test].\n",
    "          y_test: The test labels, shape [n_train] or [n_train, n_labels]\n",
    "                  (agreeing with what was passed to fit()).\n",
    "                  \n",
    "        Returns a vector if y_test is 2d, or a scalar if not.\n",
    "        \"\"\"\n",
    "        preds = self.predict(K_test)\n",
    "        y_test = torch.as_tensor(y_test)\n",
    "        assert preds.shape == y_test.shape\n",
    "        return ((preds - y_test) ** 2).mean(0)\n",
    "    \n",
    "    def mse(self, K_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes the mean squared error across outputs for a test set.\n",
    "        \n",
    "          K_test: The train-to-test kernel matrix, shape [n_train, n_test].\n",
    "          y_test: The test labels, shape [n_train] or [n_train, n_labels]\n",
    "                  (agreeing with what was passed to fit()).\n",
    "                  \n",
    "        Returns a scalar.\n",
    "        \"\"\"\n",
    "        return self.mses(K_test, y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(krr, kernel_cls=None, plot=True,\n",
    "             X_train=None, y_train=None, X_test=None, y_test=None):\n",
    "    if X_train is None:\n",
    "        X_train = toy_X_train\n",
    "    if y_train is None:\n",
    "        y_train = toy_y_train\n",
    "    if X_test is None:\n",
    "        X_test = toy_X_test\n",
    "    if y_test is None:\n",
    "        y_test = toy_y_test\n",
    "    \n",
    "    plot_xs = np.linspace(0, 1, num=500)[:, None]\n",
    "    if kernel_cls is None:\n",
    "        kernel_cls = functools.partial(RBFKernel, sigma=0.05)\n",
    "    K = kernel_cls(X_train, X_test, plot_xs)\n",
    "\n",
    "    krr.fit(K.XX, y_train)\n",
    "    train_mse = krr.mse(K.XX, y_train).item()\n",
    "    test_mse = krr.mse(K.XY, y_test).item()\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(X_train[:, 0], y_train)\n",
    "        ax.plot(plot_xs, krr.predict(K.XZ).numpy(), lw=4, color='r')\n",
    "        ax.set_title(f\"Train MSE: {train_mse:.2} / Test MSE: {test_mse:.2}\")\n",
    "    else:\n",
    "        return train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.1))\n",
    "plt.savefig('figs/toy-krr-eval.png'); plt.close()  # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what my solution looks like:\n",
    "![hi](figs/toy-krr-eval.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION CELL\n",
    "# Check that the LU solve also works; should be almost identical.\n",
    "evaluate(KernelRidgeRegression(reg_wt=.1, solver='lu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of $\\lambda$\n",
    "For this problem, we want a pretty small $\\lambda$, just enough to make it work, since the amount of inherent noise in the problem is fairly small. To check this, let's just try a bunch of different $\\lambda$s.\n",
    "\n",
    "(Note that the Cholesky solution is likely to crash with small $\\lambda$ before the `torch.solve` solution would, because \u2013 especially in the `float32` computation we're doing here \u2013\u00a0$K + n \\lambda I$ will appear singular. My results image below used the `torch.solve` version.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: What do you think will happen visually when you change $\\lambda$? Try a few different numbers for `reg_wt`, including increasing it really high (like `1000`), and see if it matches your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "for lam in lams:\n",
    "    krr = KernelRidgeRegression(reg_wt=lam)\n",
    "    krr.solver = 'lu'    # SOLUTION\n",
    "    try:\n",
    "        train, test = evaluate(krr, plot=False)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"{lam}: {e}\")\n",
    "        train = test = np.nan\n",
    "    train_mses.append(train)\n",
    "    test_mses.append(test)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lams, train_mses, marker='o', label='train MSE')\n",
    "ax.plot(lams, test_mses, marker='o', label='test MSE')\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "fig.savefig('figs/toy-krr-lambda.png'); plt.close()  # SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results, for reference, look like:\n",
    "![](figs/toy-krr-lambda.png)\n",
    "\n",
    "My Cholesky solution failed for $\\lambda < 10^{-3}$, so your plot might cut off the first two points. We can actually tell that the behavior for these small values of $\\lambda$ is numerical error, not overfitting: overfitting would increase the test error, but have the same or lower training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the kernel\n",
    "\n",
    "We can also see whether my guess at $\\sigma$ was really right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** First, what happens when you change $\\sigma$? Try some numbers, including much larger and much smaller ones, and see if it looks like what you expect to happen. See how this interacts with setting $\\lambda$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.01),\n",
    "         kernel_cls=functools.partial(RBFKernel, sigma=.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What happens if you use `LinearKernel`? `PolynomialKernel`, with different `degree`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.1),\n",
    "         kernel_cls=functools.partial(LinearKernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to `RBFKernel`, let's do a joint search over $\\lambda$ and $\\sigma$, since they'll interact with one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = [1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "sigs = [.001, .01, .05, .1, .25, .5, 1]\n",
    "mses = np.full((len(lams), len(sigs), 2), np.nan)\n",
    "\n",
    "for lam_i, lam in enumerate(lams):\n",
    "    for sig_i, sig in enumerate(sigs):\n",
    "        krr = KernelRidgeRegression(reg_wt=lam)\n",
    "        krr.solver = 'lu'  # SOLUTION\n",
    "        try:\n",
    "            mses[lam_i, sig_i, :] = evaluate(\n",
    "                krr, functools.partial(RBFKernel, sigma=sig), plot=False)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"{lam} {sig}: {e}\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(12, 4), constrained_layout=True)\n",
    "\n",
    "norm = mpl.colors.LogNorm(vmin=np.nanmin(mses), vmax=np.nanmax(mses))\n",
    "\n",
    "ax1, ax2 = axes\n",
    "ax1.matshow(mses[:, :, 0], norm=norm)\n",
    "best1, best2 = np.unravel_index(np.argmin(mses[:, :, 0]), mses.shape[:2])\n",
    "ax1.scatter([best2], [best1], marker='o', color='c', s=100)\n",
    "ax1.set_title(f\"Train MSE: min {mses[best1, best2, 0]:.3}\")\n",
    "\n",
    "im = ax2.matshow(mses[:, :, 1], norm=norm)\n",
    "best1, best2 = np.unravel_index(np.argmin(mses[:, :, 1]), mses.shape[:2])\n",
    "ax2.scatter([best2], [best1], marker='o', color='c', s=100)\n",
    "ax2.set_title(f\"Test MSE: min {mses[best1, best2, 1]:.3}\")\n",
    "fig.colorbar(im)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.xaxis.set_ticklabels([''] + [f'{sig}' for sig in sigs])\n",
    "    ax.set_xlabel(r'$\\sigma$')\n",
    "    \n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_ticklabels([''] + [f'{lam}' for lam in lams])\n",
    "    ax.set_ylabel(r'$\\lambda$')\n",
    "    \n",
    "fig.savefig('figs/toy-krr-sig-lambda.png');  # SOLUTION\n",
    "# If you get a RuntimeWarning due to nans, don't worry about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, my result looks like\n",
    "![](figs/toy-krr-sig-lambda.png)\n",
    "Smaller $\\sigma$ can get lower training error, but the best test error is definitely with $\\sigma = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting `y`\n",
    "\n",
    "What do you think will happen if we fit with `y` replaced by `y + 100`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal fit, for comparison\n",
    "evaluate(KernelRidgeRegression(reg_wt=.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with labels shifted upwards by 100\n",
    "evaluate(KernelRidgeRegression(reg_wt=.1),\n",
    "         y_train=toy_y_train + 100, y_test=toy_y_test + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Some\n",
    "\n",
    "blank\n",
    "\n",
    "space\n",
    "\n",
    "for\n",
    "\n",
    "you\n",
    "\n",
    "to\n",
    "\n",
    "think\n",
    "\n",
    "and\n",
    "\n",
    "then\n",
    "\n",
    "run\n",
    "\n",
    "it\n",
    "\n",
    "and\n",
    "\n",
    "then\n",
    "\n",
    "maybe\n",
    "\n",
    "think\n",
    "\n",
    "some\n",
    "\n",
    "more\n",
    "\n",
    "before\n",
    "\n",
    "seeing\n",
    "\n",
    "the\n",
    "\n",
    "discussion.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 2em\">\ud83e\udd14</span>\n",
    "\n",
    "Remember that in traditional linear regression, you fit $y = w \\cdot x + b$. Here, we're using $y = w \\cdot \\varphi(x)$; no $+ b$. Unlike in traditional linear regression, the kernel can more-or-less account for it, but the weird thing at the end \u2013\u00a0and the overall downward trend \u2013\u00a0could be fixed by adding an offset.\n",
    "\n",
    "The easiest way to do that is to add an additional dimension to $\\varphi(x)$ that's just a constant $1$, so that the corresponding element of $w$ can be $b$. Since $k(x, y) = \\varphi(x) \\cdot \\varphi(y)$, this means that we just add 1 to the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFKernelPlusOne(RBFKernel):\n",
    "    def _compute(self, *args):\n",
    "        return super()._compute(*args) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.1),\n",
    "         kernel_cls=functools.partial(RBFKernelPlusOne, sigma=.05),\n",
    "         y_train=toy_y_train + 100, y_test=toy_y_test + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note: traditional ridge regression regularizes with $\\lambda \\lVert w \\rVert^2$, and leaves $b$ unregularized. This has nice properties like making the fit shift _exactly_ along with an offset $y$. By adding 1 to the kernel, though, ours will _slightly_ change with offsets, because we're also effectively regularizing with $\\lambda b^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.1),\n",
    "         kernel_cls=functools.partial(RBFKernelPlusOne, sigma=.05),\n",
    "         y_train=toy_y_train + 5000, y_test=toy_y_test + 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to resolve this:\n",
    "\n",
    "1. Actually add an unregularized offset into the model and work it out properly.\n",
    "2. Mitigate the scaling by adding a large constant, instead of 1; if you add $c$, then the regularization is effectively $\\lambda {b^2}/{c^2}$.\n",
    "3. Just subtract the mean of `y_train` before passing it into the model, then add it on to predictions later.\n",
    "\n",
    "People normally do option 3 in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A harder problem\n",
    "\n",
    "Okay, that problem was too easy. Let's do something a little more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced ridge regression: Meta-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
