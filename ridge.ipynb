{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the practical component of the [Data Science Summer School](https://www.ds3-datascience-polytechnique.fr) 2019 session on \"Learning With Positive Definite Kernels: Theory, Algorithms, and Applications.\"\n",
    "Slides are available [here](https://github.com/dougalsutherland/ds3-kernels/tree/slides).\n",
    "\n",
    "It was prepared primarily by [Dougal Sutherland](http://www.gatsby.ucl.ac.uk/~dougals/), based on discussions with [Bharath Sriperumbudur](http://personal.psu.edu/bks18/), and partially based on earlier [materials](https://github.com/karlnapf/ds3_kernel_testing) by [Heiko Strathmann](http://herrstrathmann.de/).\n",
    "\n",
    "We'll cover, in varying levels of detail, the following topics:\n",
    "\n",
    "- Solving regression problems with kernel ridge regression ([`ridge.ipynb`](ridge.ipynb)):\n",
    "  - The \"standard\" approach.\n",
    "  - Computational/statistical tradeoffs using the Nystr\u00f6m and random Fourier kernel approximations.\n",
    "  - Learning an appropriate kernel function in a meta-learning setting.\n",
    "- Two-sample testing with the kernel Maximum Mean Discrepancy (MMD) ([`testing.ipynb`](testing.ipynb)):\n",
    "  - Estimators for the MMD.\n",
    "  - Learning an appropriate kernel function.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "### Colab\n",
    "\n",
    "These notebooks are available on Google Colab: [ridge](https://colab.research.google.com/github/dougalsutherland/ds3-kernels/blob/built/ridge.ipynb) or [testing](https://colab.research.google.com/github/dougalsutherland/ds3-kernels/blob/built/testing.ipynb). You don't have to set anything up yourself and it runs on cloud resources, so this is probably the easiest option. If you want to use the GPU, click Runtime -> Change runtime type -> Hardware accelerator -> GPU.\n",
    "\n",
    "### Local setup\n",
    "\n",
    "Run `check_imports_and_download.py` to see if everything you need is installed (and download some more small datasets if necessary). If that works, you're set; otherwise, read on.\n",
    "\n",
    "\n",
    "### Files\n",
    "There are a few Python files and some data files in the repository. By far the easiest thing to do is just put them all in the same directory:\n",
    "\n",
    "```\n",
    "git clone https://github.com/dougalsutherland/ds3-kernels\n",
    "```\n",
    "\n",
    "#### Python version\n",
    "This notebook requires Python 3.6+. Python 3.0 was released in 2008, and it's time to stop living in the past; most importart Python projects [are dropping support for Python 2 this year](https://python3statement.org/). If you've never used Python 3 before, don't worry! It's almost the same; for the purposes of this notebook, you probably only need to know that you should write `print(\"hi\")` since it's a function call now, and you can write `A @ B` instead of `np.dot(A, B)`.\n",
    "\n",
    "#### Python packages\n",
    "\n",
    "The main thing we use is PyTorch and Jupyter. If you already have those set up, you should be fine; just additionally make sure you also have (with `conda install` or `pip install`) `seaborn`, `tqdm`, and `sckit-learn`. We import everything right at the start, so if that runs you shouldn't hit any surprises later on.\n",
    "\n",
    "If you don't already have a setup you're happy with, we recommend the `conda` package manager - start by installing [miniconda](https://docs.conda.io/en/latest/miniconda.html). Then you can create an environment with everything you need as:\n",
    "\n",
    "```bash\n",
    "conda create --name ds3-kernels \\\n",
    "  --override-channels -c pytorch -c defaults --strict-channel-priority \\\n",
    "  python=3 notebook ipywidgets \\\n",
    "  numpy scipy scikit-learn \\\n",
    "  pytorch=1.1 torchvision \\\n",
    "  matplotlib seaborn tqdm\n",
    "\n",
    "conda activate ds3-kernels\n",
    "\n",
    "git clone https://github.com/dougalsutherland/ds3-kernels\n",
    "cd ds3-kernels\n",
    "python check_imports_and_download.py\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "(If you have an old conda setup, you can use `source activate` instead of `conda activate`, but it's better to [switch to the new style of activation](https://conda.io/projects/conda/en/latest/release-notes.html#recommended-change-to-enable-conda-in-your-shell). This won't matter for this tutorial, but it's general good practice.)\n",
    "\n",
    "(You can make your life easier when using jupyter notebooks with multiple kernels by installing `nb_conda_kernels`, but as long as you install and run `jupyter` from inside the env it will also be fine.)\n",
    "\n",
    "\n",
    "## PyTorch\n",
    "\n",
    "We're going to use PyTorch in this tutorial, even though we're not doing a ton of \"deep learning.\" (The CPU version will be fine, though a GPU might let you get slightly better performance in some of the \"advanced\" sections.)\n",
    "\n",
    "If you haven't used PyTorch before, don't worry! The API is unfortunately a little different from NumPy (and TensorFlow), but it's pretty easy to get used to; you can refer to [a cheat sheet vs NumPy](https://github.com/wkentaro/pytorch-for-numpy-users/blob/master/README.md) as well as the docs: [tensor methods](https://pytorch.org/docs/stable/tensors.html) and [the `torch` namespace](https://pytorch.org/docs/stable/torch.html#torch.eq). Feel free to ask if you have trouble figuring something out.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\DeclareMathOperator*{\\argmin}{argmin}\n",
    "\\DeclareMathOperator*{\\E}{\\mathbb E}\n",
    "\\newcommand{\\R}{\\mathbb R}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    if not os.path.exists('data/blobs.npz'):\n",
    "        !git clone https://github.com/dougalsutherland/ds3-kernels\n",
    "        os.chdir('ds3-kernels')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='notebook')\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm import tqdm  # if you're in JupyterLab/etc and this doesn't work well\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import ds3_support\n",
    "from ds3_support import as_tensors, plot_confusion_matrix, LazyKernel, pil_grid\n",
    "\n",
    "# Download some datasets. We won't need these until later; go ahead and read ahead\n",
    "# while they're downloading if you want....\n",
    "torchvision.datasets.MNIST(root='data', download=True)\n",
    "ds3_support.CombinedOmniglot(root='data', download=True)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just checking that tqdm works okay.\n",
    "# If it doesn't work, then switch to the \"from tqdm import tqdm\" line above.\n",
    "import time\n",
    "print(\"\")\n",
    "for _ in tqdm(range(100)):\n",
    "    time.sleep(.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note\n",
    "\n",
    "Please ask me for help if you get stuck! Whether it's with a PyTorch thing, some other code thing, or especially something conceptual \u2013\u00a0that's what I'm here for. Solutions are also available in [`solutions-ridge.ipynb`](solutions-ridge.ipynb), but you're better off not rushing straight to those; I'm happy to help you try to help you work things out yourself, which is probably better for you. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "Okay, time to get going.\n",
    "\n",
    "We're going to start with implementing kernel ridge regression. First, here's a toy dataset to test your code on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('data/ridge-toy.npz') as f:\n",
    "    toy_X = f['X']\n",
    "    toy_y = f['y']\n",
    "toy_X_train, toy_X_test, toy_y_train, toy_y_test = model_selection.train_test_split(\n",
    "    toy_X, toy_y, train_size=200)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(toy_X_train[:, 0], toy_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data looks like it could be well-modeled by a Gaussian RBF kernel. We'll need a fairly small bandwidth. See the minimum a little above 0.15 and the maximum a little less than 0.25? The Gaussian kernel still has a fair amount of influnce one bandwidth away, so we want to make the bandwidth a little smaller than that; say 0.05. So, first let's implement the kernel.\n",
    "\n",
    "<sup>I totally eyeballed that like I said, and definitely didn't just generate the true function in the first place from an RBF function with that bandwidth....</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first, let's implement some kernels first. I've put some helper infrastructure in the `LazyKernel` class (in [`ds3_support.kernels`](ds3_support/kernels.py)) that will be especially useful later; for now, it's just a way to organize computing it. Here's an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearKernel(LazyKernel):\n",
    "    def _compute(self, A, B):\n",
    "        return A @ B.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_compute` method computes the kernel between two inputs `A` and `B`. (`.t()` is PyTorch for taking a transpose; `@` is the nifty Python 3.6+ syntax for matrix multiplication.)\n",
    "\n",
    "The `LazyKernel` base class lets us use this in various ways. First, to find the kernel from one set of points to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(toy_X_train, toy_X_test)\n",
    "print(K)\n",
    "K.XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the X-to-X (`XX`) and Y-to-Y (`YY`) kernel matrices from the same object (which are the result of `_compute(X, X)` and `_compute(Y, Y)`). These aren't computed until you need them, but then they're cached after you use them the first time; this is why it's a `LazyKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only want the kernel matrix for a dataset to itself, you can just not pass the second argument. Then K.XY won't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearKernel(toy_X_train).XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can pass `None`, which is a special value meaning \"use the first one.\" Then `XY` and so on will exist, but it knows to cache them appropriately.\n",
    "\n",
    "You can also pass three arguments; then there'll be `XZ`, etc. You can also access them with e.g. `K[0, 2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = LinearKernel(toy_X_train, None, toy_X_test)\n",
    "print(K)\n",
    "K.YZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a slightly more complex kernel class, with some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialKernel(LazyKernel):\n",
    "    def __init__(self, X, *rest, degree=3, gamma=None, coef0=1):\n",
    "        super().__init__(X, *rest)\n",
    "        self.degree = degree\n",
    "        self.gamma = 1 / X.shape[1] if gamma is None else gamma\n",
    "        self.coef0 = coef0\n",
    "\n",
    "    def _compute(self, A, B):\n",
    "        XY = A @ B.t()\n",
    "        return (self.gamma * XY + self.coef0) ** self.degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You also totally don't need to do this \u2013\u00a0you definitely won't notice the difference on this scale of data \u2013 but if you want to cache some computation for each dataset, you can use the `_precompute` interface. You can return a list of cached information in `_precompute`, which then get passed to `_compute` as `_compute(A, *A_precomputed, B, *B_precomputed)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAndSquareKernel(LazyKernel):\n",
    "    def _precompute(self, A):\n",
    "        return [A * A]\n",
    "    \n",
    "    def _compute(self, A, A_squared, B, B_squared):\n",
    "        return A @ B.t() + A_squared @ B_squared.t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the Gaussian RBF kernel is\n",
    "$$k(X_i, X_j) = \\exp\\left( -\\frac{1}{2 \\sigma^2} \\lVert X_i - X_j \\rVert^2 \\right).$$\n",
    "\n",
    "**Exercise:** Implement that here.\n",
    "\n",
    "It might be helpful to recall that\n",
    "$$\\lVert X_i - X_j \\rVert^2 = \\lVert X_i \\rVert^2 + \\lVert X_j \\rVert^2 - 2 X_i^T X_j;$$\n",
    "you should probably convert that into a matrix form to implement it.\n",
    "\n",
    "Alternatively, [`torch.pdist`](https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pdist) computes these distances, but it only gives you the upper triangle of the matrix so you'll have to turn it into a full symmetric matrix yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFKernel(LazyKernel):\n",
    "    def __init__(self, *parts, sigma=1):\n",
    "        super().__init__(*parts)\n",
    "        self.sigma = sigma\n",
    "        # This next line doesn't really matter here, but helps a bit in the mmd stuff:\n",
    "        self.const_diagonal = 1  # Declares that k(x, x) = 1 for any x\n",
    "    \n",
    "    # TODO: implement _compute (maybe with _precompute)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation against scikit-learn's implementation (but it doesn't work in PyTorch, so don't just use it directly):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = np.random.lognormal()\n",
    "K = RBFKernel(toy_X_train, toy_X_test, sigma=sigma)\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "gamma = 1 / (2 * sigma**2)  # sklearn uses this parameterization\n",
    "assert np.allclose(K.XX.numpy(), rbf_kernel(toy_X_train, gamma=gamma))\n",
    "assert np.allclose(K.XY.numpy(), rbf_kernel(toy_X_train, toy_X_test, gamma=gamma))\n",
    "assert np.allclose(K.YY.numpy(), rbf_kernel(toy_X_test, gamma=gamma))\n",
    "\n",
    "del rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing ridge regression\n",
    "\n",
    "Okay, now we can compute all kinds of kernels; let's actually use them for something. First, let's implement ridge regression. Remember that ridge regression is\n",
    "$$\n",
    "  \\min_{f \\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^n \\lVert f(X_i) - y_i \\rVert^2 + \\frac12 \\lambda \\lVert f \\rVert_{\\mathcal{H}}^2 \n",
    ".$$\n",
    "The representer theorem tells us that $f(x) = \\sum_{i=1}^n \\alpha_i k(X_i, x)$,\n",
    "and then some calculus gives us\n",
    "$$\\alpha = (K + n \\lambda I)^{-1} y, \\tag{*}$$\n",
    "where $K_{ij} = k(X_i, X_j)$.\n",
    "\n",
    "(If you're familiar with Gaussian processes but not ridge regression, the function $f$ is exactly the posterior mean of a Gaussian process. If you're not familiar with GPs, never mind.)\n",
    "\n",
    "There's also one more wrinkle: we're going to need to compute ridge regression for multiple outputs $y$ at once with the same $X$s. This just means solving the linear system for more than one $y$ at once; most software, including PyTorch, supports this built-in.\n",
    "\n",
    "To implement $\\textrm{(*)}$, there are (at least) two approaches.\n",
    "\n",
    "- [`torch.solve`](https://pytorch.org/docs/stable/torch.html#torch.solve) is a general-purpose matrix solver, which uses an [LU decomposition](https://en.wikipedia.org/wiki/LU_decomposition). This doesn't exploit the special structure of kernel matrices (namely that they're [positive-definite](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix)).\n",
    "\n",
    "- You could also use a [Cholesky factorization](https://en.wikipedia.org/wiki/Cholesky_factorization), which is the standard approach for positive-definite matrices, and will be somewhat faster. You could implement this with [`torch.cholesky`](https://pytorch.org/docs/stable/torch.html#torch.cholesky) and [`torch.cholesky_solve`](https://pytorch.org/docs/stable/torch.html#torch.cholesky_solve) \u2013\u00a0but unfortunately PyTorch hasn't implemented derivatives through `cholesky_solve` yet, and we're going to need that later. You can use [`torch.triangular_solve`](https://pytorch.org/docs/stable/torch.html#torch.triangular_solve) instead; Cholesky gives us $L$ such that $K + n \\lambda I = L L^T$, so you'll want $\\alpha = (L L^T)^{-1} y = L^{-T} (L^{-1} y)$. (Make sure to pass `upper=False` to `triangular_solve`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression:\n",
    "    def __init__(\n",
    "        self,\n",
    "        reg_wt=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        reg_wt: The regularization weight lambda.\n",
    "        \"\"\"\n",
    "        self.reg_wt = reg_wt\n",
    "    \n",
    "    def fit(self, K_XX, y):\n",
    "        \"\"\"\n",
    "        Fit the ridge regression:\n",
    "        \n",
    "          K_XX: The training kernel matrix, of shape [n_train, n_train].\n",
    "          y: The training labels, of shape [n_train] or [n_train, n_labels].        \n",
    "        \"\"\"\n",
    "        K_XX, y = as_tensors(K_XX, y)\n",
    "        \n",
    "        assert len(K_XX.shape) == 2\n",
    "        self.n_train_ = n_train = K_XX.shape[0]\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            self.n_labels_ = None\n",
    "            y = y[:, None]\n",
    "        else:\n",
    "            assert len(y.shape) == 2\n",
    "            self.n_labels_ = y.shape[1]\n",
    "\n",
    "        assert K_XX.shape[1] == y.shape[0] == self.n_train_\n",
    "        \n",
    "        # TODO: find the solution, and save it in eg self.alpha_\n",
    "    \n",
    "    def predict(self, K_test):\n",
    "        \"\"\"\n",
    "        Predict the labels of a test set.\n",
    "        \n",
    "          K_test: The train-to-test kernel matrix, shape [n_train, n_test].\n",
    "        \n",
    "        Return: the vector of test predictions, shape [n_test].\n",
    "        \"\"\"\n",
    "        K_test = torch.as_tensor(K_test)\n",
    "        assert len(K_test.shape) == 2\n",
    "        assert K_test.shape[0] == self.n_train_\n",
    "        \n",
    "        # TODO: set preds to shape [n_test, n_labels]\n",
    "        \n",
    "        # return a vector if we were fit with a vector, matrix otherwise\n",
    "        return preds.squeeze(1) if self.n_labels_ is None else preds\n",
    "    \n",
    "    def mses(self, K_test, y_test, return_preds=False):\n",
    "        \"\"\"\n",
    "        Computes the mean squared error of each output for a test set.\n",
    "        \n",
    "          K_test: The train-to-test kernel matrix, shape [n_train, n_test].\n",
    "          y_test: The test labels, shape [n_train] or [n_train, n_labels]\n",
    "                  (agreeing with what was passed to fit()).\n",
    "                  \n",
    "        Returns a vector if y_test is 2d, or a scalar if not.\n",
    "        \"\"\"\n",
    "        preds = self.predict(K_test)\n",
    "        y_test = torch.as_tensor(y_test)\n",
    "        assert preds.shape == y_test.shape\n",
    "        errs = ((preds - y_test) ** 2).mean(0)\n",
    "        return (errs, preds) if return_preds else errs\n",
    "    \n",
    "    def mse(self, K_test, y_test, return_preds=False):\n",
    "        \"\"\"\n",
    "        Computes the mean squared error across outputs for a test set.\n",
    "        \n",
    "          K_test: The train-to-test kernel matrix, shape [n_train, n_test].\n",
    "          y_test: The test labels, shape [n_train] or [n_train, n_labels]\n",
    "                  (agreeing with what was passed to fit()).\n",
    "                  \n",
    "        Returns a scalar.\n",
    "        \"\"\"\n",
    "        errs, preds = self.mses(K_test, y_test, return_preds=True)\n",
    "        err = errs.mean()\n",
    "        return (err, preds) if return_preds else err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(krr, kernel_cls=None, plot=True,\n",
    "             X_train=None, y_train=None, X_test=None, y_test=None):\n",
    "    if X_train is None:\n",
    "        X_train = toy_X_train\n",
    "    if y_train is None:\n",
    "        y_train = toy_y_train\n",
    "    if X_test is None:\n",
    "        X_test = toy_X_test\n",
    "    if y_test is None:\n",
    "        y_test = toy_y_test\n",
    "    \n",
    "    plot_xs = np.linspace(0, 1, num=500)[:, None]\n",
    "    if kernel_cls is None:\n",
    "        kernel_cls = functools.partial(RBFKernel, sigma=0.05)\n",
    "    K = kernel_cls(X_train, X_test, plot_xs)\n",
    "\n",
    "    krr.fit(K.XX, y_train)\n",
    "    train_mse = krr.mse(K.XX, y_train).item()\n",
    "    test_mse = krr.mse(K.XY, y_test).item()\n",
    "    \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(X_train[:, 0], y_train)\n",
    "        ax.plot(plot_xs, krr.predict(K.XZ).numpy(), lw=4, color='r')\n",
    "        ax.set_title(f\"Train MSE: {train_mse:.2} / Test MSE: {test_mse:.2}\")\n",
    "    else:\n",
    "        return train_mse, test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.001))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what my solution looks like:\n",
    "![hi](figs/toy-krr-eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of $\\lambda$\n",
    "For this problem, we want a pretty small $\\lambda$, just enough to make it work, since the amount of inherent noise in the problem is fairly small. To check this, let's just try a bunch of different $\\lambda$s.\n",
    "\n",
    "(Note that the Cholesky solution is likely to crash with small $\\lambda$ before the `torch.solve` solution would, because \u2013 especially in the `float32` computation we're doing here \u2013\u00a0$K + n \\lambda I$ will appear singular. My results image below used the `torch.solve` version.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: What do you think will happen visually when you change $\\lambda$? Try a few different numbers for `reg_wt`, including increasing it really high, and see if it matches your intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1]\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "for lam in lams:\n",
    "    krr = KernelRidgeRegression(reg_wt=lam)\n",
    "    try:\n",
    "        train, test = evaluate(krr, plot=False)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"{lam}: {e}\")\n",
    "        train = test = np.nan\n",
    "    train_mses.append(train)\n",
    "    test_mses.append(test)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lams, train_mses, marker='o', label='train MSE')\n",
    "ax.plot(lams, test_mses, marker='o', label='test MSE')\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(r'$\\lambda$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My results, for reference, look like:\n",
    "![](figs/toy-krr-lambda.png)\n",
    "\n",
    "My Cholesky solution failed for $\\lambda = 10^{-7}$, so your plot might cut off the first points, or behave a little differently. In my plot at least, we can actually tell that the behavior for the smallest values of $\\lambda$ is numerical error, not overfitting: overfitting would increase the test error as we decrease $\\lambda$, but it shouldn't increase the training error. (We can see a midle abount of overfitting happening at $\\lambda = 10^{-6}$ here: train error is a little lower, but test error has ticked up a tiny bit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the kernel\n",
    "\n",
    "We can also see whether my guess at $\\sigma$ was really right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** First, what happens when you change $\\sigma$? Try some numbers, including much larger and much smaller ones, and see if it looks like what you expect to happen. See how this interacts with setting $\\lambda$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.001),\n",
    "         kernel_cls=functools.partial(RBFKernel, sigma=.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What happens if you use `LinearKernel`? `PolynomialKernel`, with different `degree`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.001),\n",
    "         kernel_cls=functools.partial(LinearKernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to `RBFKernel`, let's do a joint search over $\\lambda$ and $\\sigma$, since they'll interact with one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0]\n",
    "sigs = [.001, .01, .05, .1, .25, .5, 1]\n",
    "mses = np.full((len(lams), len(sigs), 2), np.nan)\n",
    "\n",
    "for lam_i, lam in enumerate(lams):\n",
    "    for sig_i, sig in enumerate(sigs):\n",
    "        krr = KernelRidgeRegression(reg_wt=lam)\n",
    "        try:\n",
    "            mses[lam_i, sig_i, :] = evaluate(\n",
    "                krr, functools.partial(RBFKernel, sigma=sig), plot=False)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"{lam} {sig}: {e}\")\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(8, 4), constrained_layout=True)\n",
    "\n",
    "norm = mpl.colors.LogNorm(vmin=np.nanmin(mses), vmax=np.nanmax(mses))\n",
    "\n",
    "ax1, ax2 = axes\n",
    "ax1.matshow(mses[:, :, 0], norm=norm)\n",
    "best1, best2 = np.unravel_index(np.argmin(mses[:, :, 0]), mses.shape[:2])\n",
    "ax1.scatter([best2], [best1], marker='o', color='c', s=100)\n",
    "ax1.set_title(f\"Train MSE: min {mses[best1, best2, 0]:.3}\")\n",
    "\n",
    "im = ax2.matshow(mses[:, :, 1], norm=norm)\n",
    "best1, best2 = np.unravel_index(np.argmin(mses[:, :, 1]), mses.shape[:2])\n",
    "ax2.scatter([best2], [best1], marker='o', color='c', s=100)\n",
    "ax2.set_title(f\"Test MSE: min {mses[best1, best2, 1]:.3}\")\n",
    "fig.colorbar(im)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.grid(False)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.xaxis.set_ticklabels([''] + [f'{sig}' for sig in sigs])\n",
    "    ax.set_xlabel(r'$\\sigma$')\n",
    "    \n",
    "    ax.yaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_ticklabels([''] + [f'{lam}' for lam in lams])\n",
    "    ax.set_ylabel(r'$\\lambda$')\n",
    "    \n",
    "# If you get a RuntimeWarning due to nans, don't worry about it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, my result looks like\n",
    "![](figs/toy-krr-sig-lambda.png)\n",
    "Smaller $\\sigma$ can get lower training error, but the best test error is definitely with $\\sigma = 0.05$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shifting `y`\n",
    "\n",
    "What do you think will happen if we fit with `y` replaced by `y + 100`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal fit, for comparison\n",
    "evaluate(KernelRidgeRegression(reg_wt=.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with labels shifted upwards by 100\n",
    "evaluate(KernelRidgeRegression(reg_wt=.001),\n",
    "         y_train=toy_y_train + 100, y_test=toy_y_test + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"font-size: 3em\">\ud83e\udd14</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"font-size: 3em\">\ud83e\udd14</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<span style=\"font-size: 3em\">\ud83e\udd14</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in traditional linear regression, you fit $y = w \\cdot x + b$. Here, we're using $y = w \\cdot \\varphi(x)$; no $+ b$. Unlike in traditional linear regression, the kernel can more-or-less account for it, but the weird thing at the end \u2013\u00a0and the overall downward trend \u2013\u00a0could be fixed by adding an offset.\n",
    "\n",
    "The easiest way to do that is to add an additional dimension to $\\varphi(x)$ that's just a constant $1$, so that the corresponding element of $w$ can be $b$. Since $k(x, y) = \\varphi(x) \\cdot \\varphi(y)$, this means that we just add 1 to the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFKernelPlusOne(RBFKernel):\n",
    "    def _compute(self, *args):\n",
    "        return super()._compute(*args) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.001),\n",
    "         kernel_cls=functools.partial(RBFKernelPlusOne, sigma=.05),\n",
    "         y_train=toy_y_train + 100, y_test=toy_y_test + 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to note: traditional ridge regression regularizes with $\\lambda \\lVert w \\rVert^2$, and leaves $b$ unregularized. This has nice properties like making the fit shift _exactly_ along with an offset $y$. By adding 1 to the kernel, though, ours will _slightly_ change with offsets, because we're also effectively regularizing with $\\lambda b^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(KernelRidgeRegression(reg_wt=.001),\n",
    "         kernel_cls=functools.partial(RBFKernelPlusOne, sigma=.05),\n",
    "         y_train=toy_y_train + 5000, y_test=toy_y_test + 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to resolve this:\n",
    "\n",
    "1. Actually add an unregularized offset into the model and work it out properly.\n",
    "2. Mitigate the scaling by adding a large constant, instead of 1; if you add $c$, then the regularization is effectively $\\lambda {b^2}/{c^2}$.\n",
    "3. Just subtract the mean of `y_train` before passing it into the model, then add it on to predictions later.\n",
    "\n",
    "People normally do option 3 in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A harder problem\n",
    "\n",
    "Okay, that problem was too easy. Let's do something slighty more interesting.\n",
    "\n",
    "Not _too_ interesting, though; we'll start with the MNIST dataset of handwritten images, with the goal of identifying which digit an image is. (We're building up!)\n",
    "\n",
    "But this is a classification dataset, and we're doing ridge regression right now, you say!\n",
    "\n",
    "That's true. You'd probably do better to use an actual classification loss. But we're going to just do regression to \"one-hot\" labels, that is the label for an image of a handwritten 0 will be\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    ".$$\n",
    "We're going to use our multi-output support for RidgeRegression that we implemented before to train 10 different regression models: one that predicts the degree of zero-ness of an image, one for the one-ness, etc.\n",
    "\n",
    "(This is sometimes called the [Brier score](https://en.wikipedia.org/wiki/Brier_score#Original_definition_by_Brier), and it is a [proper scoring rule](https://en.wikipedia.org/wiki/Scoring_rule#ProperScoringRules). So, yes, you could get better performance from other losses, but this one is at least well-posed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some utilities for pytorch datasets\n",
    "\n",
    "def read(ds, batch_size=None, **kwargs):\n",
    "    if batch_size is None:\n",
    "        batch_size = len(ds)\n",
    "    return next(iter(DataLoader(ds, batch_size=batch_size, **kwargs)))\n",
    "\n",
    "def random_subset(ds, n):\n",
    "    return torch.utils.data.random_split(ds, [n, len(ds) - n])[0]\n",
    "\n",
    "MNIST = functools.partial(\n",
    "    torchvision.datasets.MNIST,\n",
    "    root='data',\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        lambda t: t.reshape(784),\n",
    "    ]),\n",
    "    target_transform=lambda y:\n",
    "        torch.nn.functional.one_hot(torch.as_tensor(y), num_classes=10).float()\n",
    ")\n",
    "\n",
    "mnist_train = MNIST(train=True)\n",
    "mnist_test = MNIST(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just look at some data points\n",
    "X, y = read(mnist_train, batch_size=100, shuffle=True)\n",
    "pil_grid(X.reshape(-1, 1, 28, 28), nrow=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST has 60,000 images. Even constructing a, say, 50,000 $\\times$ 50,000 matrix is too much, so we'll turn to the approximations in a bit. Let's try training on a random subset first, though, to (a) check that our multi-output implementation works, and (b) set a baseline to compare to for the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a subset of the data\n",
    "train_X, train_y = read(mnist_train, 5_000, shuffle=True)\n",
    "test_X, test_y = read(mnist_test, 5_000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_eval(truth, preds, label_names=None):\n",
    "    if label_names is None:\n",
    "        label_names = np.arange(truth.shape[1]).astype(str)\n",
    "        \n",
    "    # want integers instead of one-hot\n",
    "    truth = truth.argmax(1)\n",
    "    preds = preds.argmax(1)\n",
    "    \n",
    "    ax = plot_confusion_matrix(truth, preds, label_names, rotation=0, figsize=(7, 6))\n",
    "    fig = ax.figure\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    report = sklearn.metrics.classification_report(\n",
    "        truth, preds, target_names=label_names)\n",
    "    display(HTML(f'<pre>{report}</pre>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = RBFKernelPlusOne(train_X, test_X)\n",
    "\n",
    "krr = KernelRidgeRegression(reg_wt=1e-5)\n",
    "krr.fit(K.XX, train_y)\n",
    "test_preds = krr.predict(K.XY)\n",
    "\n",
    "clf_eval(test_y, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah....that wasn't great. We forgot to set the kernel bandwidth.\n",
    "\n",
    "A usual heuristic to start with is the median distance between data points.\n",
    "\n",
    "**Exercise:** compute the median distance between training data points. You can either compute this yourself, or use predefined functions, whichever you'd prefer. If it's too slow to compute, you can subsample the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find med_sigma\n",
    "med_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = RBFKernelPlusOne(train_X, test_X, sigma=med_sigma)\n",
    "\n",
    "krr = KernelRidgeRegression(reg_wt=1e-5)\n",
    "krr.fit(K.XX, train_y)\n",
    "test_preds = krr.predict(K.XY)\n",
    "\n",
    "clf_eval(test_y, test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! I got 96% accuracy, which is pretty reasonable. (You can do better with a convnet, of course, but not bad for such a simple model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these predictions aren't actually valid probabilities, which is slightly annoying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_preds.numpy().ravel(), bins='auto', histtype='stepfilled');\n",
    "plt.axvline(0, color='r')\n",
    "plt.axvline(1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is the median really the best choice? And what about the arbitrary choice of regularization weight?\n",
    "\n",
    "We could do a grid search like before, but let's do something more fun: gradient descent!\n",
    "\n",
    "We want to find the derivative of the validation error with respect to $\\sigma$ and $\\lambda$. That is,\n",
    "$$\n",
    "\\nabla_{\\sigma\\lambda} \\frac{1}{n^\\text{val}} \\sum_{i=1}^{n^\\text{val}} \\lVert\n",
    "    \\hat{y}_{X_\\text{train},\\sigma,\\lambda}(X_i^\\text{val}) - y_i^\\text{val}\n",
    "\\rVert^2\n",
    ",$$\n",
    "where $\\hat{y}_{X_\\text{train},\\sigma,\\lambda}$ is the predictor trained with $X_\\text{train}$ using kernel bandwidth $\\sigma$ and regularization $\\lambda$.\n",
    "\n",
    "(Although $\\hat y$ fundamentally uses the square loss, we could conceivably use another \"outer\" loss. But since $\\hat y$ isn't a distribution, we can't use cross-entropy or similar losses, so square loss is a reasonable choice. If we really wanted, we could e.g. learn an affine transformation of these outputs and then apply a softmax to them to get valid probabilities in the end.)\n",
    "\n",
    "Because we have a PyTorch expression for the whole process of $\\hat{y}$, we can just take derivatives. We'll paramaterize with $\\log \\lambda$ and $\\log \\sigma$, though, to avoid invalid values (and because it's probably a better domain for each anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# You can use a GPU here if you want, but you have to have been careful about\n",
    "# managing the device of tensors in your implementation or it might crash.\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "log_lam = torch.tensor(np.log(1e-4), requires_grad=True, device=device)\n",
    "log_sig = torch.tensor(np.log(med_sigma), requires_grad=True, device=device)\n",
    "opt = torch.optim.SGD([log_lam, log_sig], lr=1)\n",
    "\n",
    "trace = []\n",
    "\n",
    "n_steps = 100 # Feel free to increase...\n",
    "with tqdm(range(n_steps)) as bar:\n",
    "    for i in bar:\n",
    "        opt.zero_grad()  # reset gradient state before computing things\n",
    "        \n",
    "        lam = torch.exp(log_lam)\n",
    "        sig = torch.exp(log_sig)\n",
    "\n",
    "        # Fitting on the full training set is a little slow,\n",
    "        # so let's subsample.\n",
    "        # Also, since we're optimizing parameters based on fit performance,\n",
    "        # we should evaluate on points from the *training set*.\n",
    "        \n",
    "        batches = iter(DataLoader(mnist_train, batch_size=500, shuffle=True))\n",
    "        sub_train_X, sub_train_y = [x.to(device) for x in next(batches)]\n",
    "        sub_val_X, sub_val_y = [x.to(device) for x in next(batches)]\n",
    "        \n",
    "        # compute the loss on the validation set\n",
    "        krr = KernelRidgeRegression(reg_wt=lam)\n",
    "        K = RBFKernelPlusOne(sub_train_X, sub_val_X, sigma=sig)\n",
    "        krr.fit(K.XX, sub_train_y)\n",
    "        loss = krr.mse(K.XY, sub_val_y)\n",
    "\n",
    "        loss.backward()  # compute the gradients\n",
    "        opt.step()       # update lam / sig following the gradients\n",
    "        lam_val = lam.cpu().item()\n",
    "        sig_val = sig.cpu().item()\n",
    "        loss_val = loss.cpu().item()\n",
    "        trace.append((i, lam_val, sig_val, loss_val))\n",
    "        bar.set_postfix(lam=f\"{lam_val:.4}\", sig=f\"{sig_val:.4}\", loss=f\"{loss_val:.4}\")\n",
    "\n",
    "fig, (a1, a2, a3) = plt.subplots(ncols=3, figsize=(16, 4), constrained_layout=True)\n",
    "inds, lams, sigs, losses = np.asarray(trace).T\n",
    "\n",
    "a1.plot(inds, losses)\n",
    "a1.set_title(\"MSE\")\n",
    "\n",
    "a2.plot(inds, lams)\n",
    "a2.set_title(r\"$\\lambda$\")\n",
    "a2.set_yscale('log')\n",
    "\n",
    "a3.plot(inds, losses)\n",
    "a3.set_title(r\"$\\sigma$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it actually does better with a slightly smaller bandwidth (and a smaller $\\lambda$).\n",
    "\n",
    "One caveat to this is that the optimal parameters will generally depend on $n$; here we're finding the best parameters for a fit on 500 training points, but you might want smaller $\\lambda$ and $\\sigma$ as $n$ grows.\n",
    "\n",
    "Since the loss landscape is surely nonconvex, it might be better to combine grid search and local gradient descent, starting from various locations. We could of course try other optimizers too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Did this optimization help the MSE that it's optimizing on the test set (`mnist_test`) versus our heuristic guess before? What about the classification accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: blue\">At this point, you've finished the baseline of this chunk of the tutorial.<sup>(Don't you feel smarter?)</sup></span>\n",
    "    \n",
    "<span style=\"color: blue\">There are now three more-or-less independent sections remaining for the ridge regression component; feel free to do whichever you feel like of them, in any order.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further exploration of the kernel for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more open-ended questions you might want to explore at this point (but also consider going on to the remaining sections of the tutorial.\n",
    "\n",
    "**Note:** MNIST might be too easy to really look into these (since we're already at 96% accuracy with the most basic thing). You could try `torchvision.datasets.FashionMNIST` or `torchvision.datasets.CIFAR10`. (Pass `root='data', download=True` to get them if necessary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try other kernels than the Gaussian RBF on this data. Some that might be interesting:\n",
    "\\begin{align}\n",
    "\\exp\\left( -\\frac12 \\left(\\frac{\\lVert x - y \\rVert}{\\ell}\\right)^\\beta \\right)\n",
    "&\\qquad\\text{ for $0 < \\beta \\le 2$ ($\\beta = 2$ is Gaussian, $\\beta = 1$ is Laplacian)}\n",
    "\\\\\n",
    "\\lVert x - z_0 \\rVert + \\lVert y - z_0 \\rVert - \\lVert x - y \\rVert\n",
    "&\\qquad\\text{ for any $z_0$ (\"distance kernel\")}\n",
    "\\\\\n",
    "\\frac{1}{\\sqrt{\\lVert x - y \\rVert^2 + c}}\n",
    "&\\qquad\\text{ (inverse multiquadric)}\n",
    "\\\\\n",
    "\\left( 1 + \\frac{\\lVert x - y \\rVert^2}{2 \\alpha \\ell^2} \\right)^{-\\alpha}\n",
    "&\\qquad\\text{ (rational quadratic; $\\alpha \\to \\infty$ becomes Gaussian)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Does it work if you start at a bad value of the hyperparameters? Is this behavior different between choices of kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** All of the kernels above are based on plain Euclidean distances between images. What if you did something different? For example, $\\lVert A (x - y) \\rVert$ for a learned transformation matrix $A$?\n",
    "\n",
    "**Exercise:** What about $k(x, y) = \\kappa( \\phi(x), \\phi(y) )$, where $\\kappa$ is one of the kernels above and $\\phi$ is a learned network? (We'll also explore this a little bit in the meta-learning section below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced kernel ridge regression: kernel approximation\n",
    "\n",
    "Because MNIST is too big to fit directly, we were just subsampling.\n",
    "\n",
    "In the lecture, we discussed two approximations for this setting:\n",
    "\n",
    "- _Nystr\u00f6m_, instead of finding $f(x) = \\sum_{i=1}^n \\alpha_i k(X_i, x)$,\n",
    "finds $f(x) = \\sum_{i=1}^m \\alpha_i k(\\tilde X_i, x)$.\n",
    "You can pick the $\\tilde X_i$ in various ways:\n",
    "uniformly,\n",
    "according to a $k$-means clustering on the original $X_i$,\n",
    "by approximate leverage scores,\n",
    "or more.\n",
    "You could even optimize the $\\tilde X_i$ with gradient descent if you wanted,\n",
    "since we know how to do that;\n",
    "that makes the model something like a classical RBF net.\n",
    "\n",
    "- _Random Fourier features_ find a linear function in the feature space $\\cos(\\omega_i^T x)$, $\\sin(\\omega_i^T x)$, where the $\\omega$ are sampled from the Fourier transform of the kernel. For a Gaussian RBF kernel with bandwidth $\\sigma$, this is a Gaussian distribution with mean 0 and variance $\\frac{1}{\\sigma^2} I$.\n",
    "\n",
    "**Exercise:** Implement and compare ridge regression with Nystr\u00f6m and random Fourier features to the full kernel ridge regression based on subsampled data we've been using so far. You'll probably want to implement a \"primal\" ridge regression class, based on\n",
    "$$\\hat w = (X^T X + \\lambda I)^{-1} X^T y;$$\n",
    "here $X^T X$ is the covariance matrix, while $X X^T$ would be the kernel matrix.\n",
    "Once you have that working, you can make subclasses to handle the different kinds of features. \n",
    "\n",
    "**Exercise:** How does each method \u2013\u00a0Nystr\u00f6m, RFF, just subsampling \u2013 improve in accuracy as you give it more computation budget?  (A simple way to approximate computation would be to equalize the size of the basis / training subset. Or you could actually measure the time it takes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced ridge regression: Meta-learning\n",
    "\n",
    "<span style=\"color: red\">You probably want a GPU for this section. You can use Colab; see the link at the top.</span> If you're switching to Colab, you can either upload your version of this notebook or just copy-paste over your kernel class and KernelRidgeRegression implementation.\n",
    "\n",
    "MNIST has only 10 categories of image, and thousands of examples of each. It's not the most interesting of problems.\n",
    "\n",
    "[Omniglot](https://github.com/brendenlake/omniglot) is \"MNIST transpose\": it has 20 examples each of 1,623 characters (from 50 different alphabets).\n",
    "\n",
    "![](figs/omniglot_grid.jpg)\n",
    "\n",
    "Since there are only 20 examples per class, simple methods like ridge regression with Gaussian kernels aren't going to work very well. But humans are generally able to recognize these categories, even from an alphabet they've never seen before, based on one or a few examples.\n",
    "\n",
    "One popular problem in this area is called meta-learning, \"learning to learn\" \u2013 to find a learning algorithm such that given a few examples of a new character, we can quickly learn to recognize it.\n",
    "\n",
    "You might guess, based on what notebook this is coming at the end of, that one way to do this is to learn a kernel for ridge regression. The goal is to find a kernel such that, given a new type of character, you can quickly learn an effective classifier. (This was recently done by [Bertinetto et al. (ICLR 2019)](http://arxiv.org/abs/1805.08136); they called it Ridge Regression Differentiable Discriminator, R2-D2.)\n",
    "\n",
    "Specifically, we're going to use a \"deep kernel\" of the form\n",
    "$$\n",
    "k(x, y) = \\kappa( \\phi(x), \\phi(y) )\n",
    ",$$\n",
    "where $\\phi$ is a deep network and $\\kappa$ is some standard fixed kernel. Bertinetto et al. used $\\kappa(a, b) = a^T b$, putting all the work on the deep network $\\phi$, but we can also try playing around with other choices for $\\kappa$. Bertinetto et al. used a four-layer convolutional $\\phi$, implemented in `ds3_support.R2D2Featurizer`. (It gives 3,584-dimensional  features, so a Gaussian kernel on top might have some issues.)\n",
    "\n",
    "The training procedure is set up in \"episodes\":\n",
    "- Sample $N$ classes from the dataset.\n",
    "- Train ridge regression on $K$ examples (\"shots\") from each class.\n",
    "- Test on $Q$ test examples (\"queries\") from each class.\n",
    "- Update the learner parameters (kernel weights, regularization $\\lambda$) to follow the gradient of this test loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following Vinyals et al. (2016), Bertinetto et al. augment Omniglot\n",
    "# by adding classes corresponding to rotations of the other classes.\n",
    "# This class does that.\n",
    "\n",
    "sz = 28\n",
    "omniglot = ds3_support.CombinedOmniglot(\n",
    "    'data',\n",
    "    transform=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((sz, sz)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        lambda t: 1 - t,  # make it white-on-black like MNIST\n",
    "    ]),\n",
    "    rotations=[0, 90, 180, 270],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_r = omniglot.n_rotations\n",
    "for i in [967, 1003]:\n",
    "    rots = torch.utils.data.ConcatDataset([\n",
    "        omniglot.class_subset(omniglot.construct_class_id(i, r)) for r in range(n_r)\n",
    "    ])\n",
    "    X, y = read(rots)\n",
    "    assert (omniglot.decompose_class_id(y)[0] == i).all()\n",
    "    display(pil_grid(X, nrow=omniglot.n_per_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_episode(n_shots=5, n_classes=60, total_batch_size=600):\n",
    "    per_class = total_batch_size // n_classes\n",
    "    n_extra = omniglot.n_per_class - per_class\n",
    "    assert n_extra >= 0\n",
    "    assert total_batch_size % n_classes == 0\n",
    "    assert n_shots < per_class\n",
    "    \n",
    "    base_class_ids = np.random.choice(training_base_classes, size=n_classes, replace=False)\n",
    "    rotation_ids = np.random.randint(omniglot.n_rotations, size=n_classes)\n",
    "    class_ids = [\n",
    "        omniglot.construct_class_id(b, r) for b, r in zip(base_class_ids, rotation_ids)\n",
    "    ]\n",
    "    n_shots = np.random.choice([1, 5])\n",
    "    n_query = per_class - n_shots\n",
    "\n",
    "    parts = [\n",
    "        torch.utils.data.random_split(\n",
    "            omniglot.class_subset(class_id),\n",
    "            [int(n) for n in [n_shots, n_query, n_extra]]\n",
    "            # these are np.int64s, need to make ints...sigh\n",
    "        )[:2]\n",
    "        for class_id in class_ids\n",
    "    ]\n",
    "    train_ds, query_ds = [torch.utils.data.ConcatDataset(ps) for ps in zip(*parts)]\n",
    "\n",
    "    train_X, train_y = read(train_ds)\n",
    "    query_X, query_y = read(query_ds)\n",
    "    \n",
    "    return train_X, train_y, query_X, query_y, np.asarray(class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_onehot(y, *other_ys):\n",
    "    # We want one-hot matrices, but only for the labels we actually have.\n",
    "    onehotter = sklearn.preprocessing.OneHotEncoder(\n",
    "        categories='auto', dtype=np.float32, sparse=False)\n",
    "    y = torch.as_tensor(onehotter.fit_transform(y[:, None]))\n",
    "    other_ys = [torch.as_tensor(onehotter.transform(oth[:, None]), device=device)\n",
    "                for oth in other_ys]\n",
    "    \n",
    "    def convert_back(labels):\n",
    "        \"Converts integers in the onehot output space to integers in onehot input space.\"\n",
    "        return torch.as_tensor(onehotter.categories_[0], device=labels.device)[labels]\n",
    "    \n",
    "    return [y] + other_ys, convert_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the featurizing kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_base_classes, testing_base_classes = model_selection.train_test_split(\n",
    "    np.arange(omniglot.n_base_classes), test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = ds3_support.R2D2Featurizer().to(device)\n",
    "\n",
    "def composite_kernel(*parts):\n",
    "    return LinearKernel(*[featurizer(p) for p in parts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** implement the fitting and training loop, using the `load_episode` helper above. You might find the `convert_to_onehot` function useful as well, and the `convert_back` function it returns could be helpful to compute accuracies.\n",
    "\n",
    "Hint: If you're not super experienced in PyTorch, the training loop for optimizng `sigma`/`lambda` above might be a useful reference. When constructing the optimizer, `list(featurizer.parameters())` gives you a list of all the parameters inside `featurizer`. You might also want to try `torch.optim.Adam` instead of `SGD` (or not).\n",
    "\n",
    "If you get errors about `cuda.FloatTensor` versus regular `FloatTensor`s, you might have to add a `device` argument somewhere, e.g. to the `torch.eye` inside your `KernelRidgeRegression.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lam = torch.tensor(0.1, requires_grad=True, device=device)\n",
    "opt = torch.optim.Adam([log_lam] + list(featurizer.parameters()), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(range(1)) as bar:  # obviously you should run for more than one step :)\n",
    "    for episode_i in bar:\n",
    "        # load an episode\n",
    "        train_X, train_y, query_X, query_y, class_ids = load_episode(\n",
    "            n_shots=np.random.choice([1, 5]),\n",
    "            total_batch_size=600,\n",
    "            n_classes=60,\n",
    "        )\n",
    "        train_X = train_X.to(device)\n",
    "        query_X = query_X.to(device)\n",
    "        (train_y_onehot, query_y_onehot), convert_back = convert_to_onehot(train_y, query_y)\n",
    "        train_y_onehot = train_y_onehot.to(device)\n",
    "        query_y_onehot = query_y_onehot.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        K = composite_kernel(train_X, query_X)\n",
    "        krr = KernelRidgeRegression(reg_wt=torch.exp(log_lam))\n",
    "        krr.fit(K.XX, train_y_onehot)\n",
    "        loss, preds = krr.mse(K.XY, query_y_onehot, return_preds=True)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # compute accuracy\n",
    "        preds_cls = convert_back(preds.argmax(1))\n",
    "        acc = (preds_cls == query_y).sum().float() / query_y.shape[0]\n",
    "        \n",
    "        bar.set_postfix(loss=loss.item(), lam=torch.exp(log_lam).item(), acc=acc.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Play around with different kernels, including different featurizers. The source for `R2D2Featurizer` is in [`ds3_support/r2d2_featurizer.py`](ds3_support/r2d2_featurizer.py) if you want to build off that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you have a decent GPU and some time:** Try a more complicated dataset; miniImageNet or CIFAR-FS. I didn't set these up for you, so this will take more effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Much more complicated exercise:** A more effective way to do meta-learning for classification \u2013 that still uses kernels! \u2013\u00a0is [MetaOptNet](https://arxiv.org/abs/1904.03758). In that case, they use SVM classifiers, where the solver is not easy to differentiate through \u2013\u00a0but you can still find the derivative, because it's a convex problem, and you can differentiate the KKT conditions. Read up on this approach, and maybe try implementing it and playing around. (Their code, in PyTorch, is at [`kjunelee/MetaOptNet`](https://github.com/kjunelee/MetaOptNet).)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0cd62862c6fc424cb2d36f7d0dcbdf31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c289c181ec244fc89eda9616e61c7857",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_f011cf2a6d3e47788365f6f07f57eaea",
       "value": "100% 1/1 [00:05&lt;00:00,  5.89s/it, acc=0.757, lam=1.1, loss=0.0123]"
      }
     },
     "1e4685d3ed6f45abb8dcf6911b628f06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2aaa47119b074741b0aced06b0199f1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2dbfe1ed9543488188e73c9fb9eafd6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3e2e662893894879b932b147ff7fbd59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "497a643903ae41c7ad0229da8e4c042b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7f41edfa17be41dd86247fc956457058",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_64ac9590ab224b22974bec76c88ce1b4",
       "value": 100
      }
     },
     "4c34a8bd2f0640e5b9a17cd73dffa410": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "52f55f8fb1954d4e88492ad77a128d03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f1b286422c7742b48c76916047dacefa",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_1e4685d3ed6f45abb8dcf6911b628f06",
       "value": "100% 100/100 [00:19&lt;00:00,  5.11it/s, lam=9.536e-05, loss=0.02538, sig=5.45]"
      }
     },
     "556b6ac02c6d4aa88dbc8e5ae61ead1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4c34a8bd2f0640e5b9a17cd73dffa410",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2dbfe1ed9543488188e73c9fb9eafd6a",
       "value": 1
      }
     },
     "570edeb3a4ef4e1282c477ecfb5c59a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64ac9590ab224b22974bec76c88ce1b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7edd064911e043c6a774ae242a240ebe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7f41edfa17be41dd86247fc956457058": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "84fc35101e7743bebee8b62b9e8744da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2aaa47119b074741b0aced06b0199f1d",
       "placeholder": "\u200b",
       "style": "IPY_MODEL_7edd064911e043c6a774ae242a240ebe",
       "value": "100% 100/100 [00:01&lt;00:00, 89.88it/s]"
      }
     },
     "8b48d3b50b34479a87bdeaf70592c7f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9a76d26372ff447bb0f9b4d031b9bc47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f44549a443a64698bcd4826670283370",
       "max": 100,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3e2e662893894879b932b147ff7fbd59",
       "value": 100
      }
     },
     "bfb5425459ac41e1a5c539e9cdbe13d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_497a643903ae41c7ad0229da8e4c042b",
        "IPY_MODEL_84fc35101e7743bebee8b62b9e8744da"
       ],
       "layout": "IPY_MODEL_eaaff76c0ebd47b093583e6405562914"
      }
     },
     "c289c181ec244fc89eda9616e61c7857": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d4f25146071443e280f12c4264f41420": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9a76d26372ff447bb0f9b4d031b9bc47",
        "IPY_MODEL_52f55f8fb1954d4e88492ad77a128d03"
       ],
       "layout": "IPY_MODEL_570edeb3a4ef4e1282c477ecfb5c59a5"
      }
     },
     "e9c51305b3c4412f9901b48f584e4a8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_556b6ac02c6d4aa88dbc8e5ae61ead1b",
        "IPY_MODEL_0cd62862c6fc424cb2d36f7d0dcbdf31"
       ],
       "layout": "IPY_MODEL_8b48d3b50b34479a87bdeaf70592c7f7"
      }
     },
     "eaaff76c0ebd47b093583e6405562914": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f011cf2a6d3e47788365f6f07f57eaea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f1b286422c7742b48c76916047dacefa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f44549a443a64698bcd4826670283370": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}